{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laeti\\SHK_NODE\\filter_sparsity\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys \n",
    "import os\n",
    "\n",
    "# Setup paths\n",
    "PROJECT_ROOT = os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), 'filter_sparsity')\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "    \n",
    "print(PROJECT_ROOT)\n",
    "\n",
    "from polar_ns.train import fit_model\n",
    "from polar_ns.prune import main\n",
    "from polar_ns.fine_tune import fine_tune_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For NS: use lbd/s/sr as lambda value\n",
    "# %% crreate lambda space\n",
    "lambda_min = 1e-4\n",
    "#lambda_min = 0.0014\n",
    "lambda_max = 5\n",
    "lambda_seq_len = 10\n",
    "lambda_seq = np.exp(np.linspace(np.log(lambda_max), np.log(lambda_min), lambda_seq_len))\n",
    "lambda_seq = np.concatenate([lambda_seq, [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.00000000e+00, 1.50266519e+00, 4.51600535e-01, 1.35720881e-01,\n",
       "       4.07886087e-02, 1.22583245e-02, 3.68403150e-03, 1.10717318e-03,\n",
       "       3.32742119e-04, 1.00000000e-04, 0.00000000e+00])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config for pp via loss type = \"sr\"\n",
    "def load_config_train(lbd, path_sv, path_bckp, path_log):\n",
    "    config_train = {\n",
    "    #'config' : [6, 'A', 16, 'A'],\n",
    "    'loss' : 'sr', \n",
    "    'lbd' : lbd, \n",
    "    #'alpha' : 1, \n",
    "    #'t' : 1,\n",
    "    'epochs' : 50, \n",
    "    'batch_size' : 256, \n",
    "    'test_batch_size' : 256,\n",
    "    'max_epoch' : None, \n",
    "    'lr' : 0.15, \n",
    "    'momentum' : 0.9, \n",
    "    'weight_decay': 0.0, \n",
    "    'resume' : None,\n",
    "    'no_cuda': True, \n",
    "    'seed' : 1234, \n",
    "    'log_interval' : 10,\n",
    "    'bn_init_value' : 0.5, \n",
    "    # only used for pp\n",
    "    'clamp' : 1.0, \n",
    "    'gate' : False, \n",
    "    'flops_weighted' : False,\n",
    "    'weight-max': None, \n",
    "    'weight-min' : None, \n",
    "    'bn_wd' : True, \n",
    "    'target-flops' : None, \n",
    "    'debug' : False,\n",
    "    'arch' : 'leNet', \n",
    "    'retrain' : False, \n",
    "    'save' : path_sv, \n",
    "    'backup' : path_bckp, \n",
    "    'log' : path_log\n",
    "    #'save' : './checkpoints_ns/', \n",
    "    #'backup' : './backup_ns/', \n",
    "    #'log' : './events_ns/'\n",
    "    }\n",
    "    \n",
    "    return config_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config_prune(path_model, path_save):\n",
    "    config_prune = {\n",
    "    'model' : path_model,\n",
    "    'batch_size': 256, \n",
    "    'test_batch_size' : 256,\n",
    "    'no_cuda': True, \n",
    "    # Type of pruning/where to prune on; 'polarization', 'l1-norm', 'ns'\n",
    "    'prune_type': 'ns', \n",
    "    # How to find prunish threshold; only applied if prune_type == ploarization; [\"fixed\", \"grad\", \"search\"]; for og. PP: grad\n",
    "    'pruning_strategy' : None,\n",
    "    #'Pruning ratio of the L1-Norm'; only applied if prune_type == l1-norm OR ns, default none --> ratio to be pruned\n",
    "    'l1_norm_ratio': 0.1,  \n",
    "    # l1_norm_cutoff for other ns and pp threshold finding --> IMPLEMENTED ; CAN BE USED!\n",
    "    'l1_norm_cutoff': None,\n",
    "    # None` or `\"default\"`: default behaviour. The pruning threshold is determined by `sparse_layer\n",
    "    'prune_mode' : 'default', \n",
    "    'save' : './checkpoints_ns/',\n",
    "    'gate' : False\n",
    "    }\n",
    "    return config_prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config_finetune(path_sv, path_bckp, path_log, path_refine):\n",
    "    config_finetune = {\n",
    "    #'config' : [6, 'A', 16, 'A'],\n",
    "    'cuda' : False, \n",
    "    'no_cuda' : True, \n",
    "    # only relevant if sr TRUE, sr = sparsity regularization --> same as lbd in main\n",
    "    's' : 0.0001, \n",
    "    'sr' : False, # no further sr \n",
    "    'epochs' : 50, \n",
    "    'batch_size' : 256, \n",
    "    'test_batch_size' : 256,\n",
    "    'max_epoch' : None, \n",
    "    'lr' : 0.15, \n",
    "    'momentum' : 0.9, \n",
    "    'weight_decay': 0.0, \n",
    "    'seed' : 1234, \n",
    "    'log_interval' : 10,\n",
    "    'gate' : False, \n",
    "    'flops_weighted' : False,\n",
    "    'bn_wd' : True, \n",
    "    'resume' : None,\n",
    "    'arch' : 'leNet', \n",
    "    #'refine' : './checkpoints/pruned_grad.pth.tar',\n",
    "    'refine': path_refine,\n",
    "    'save' : path_sv,\n",
    "    'backup' : path_bckp,\n",
    "    'log' : path_log}\n",
    "    return config_finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_paths(lbd): \n",
    "    save =  './checkpoints_ns_' + str(lbd) +'/'\n",
    "    backup = './backup_ns_'+ str(lbd) +'/'\n",
    "    log = './events_ns_/' + str(lbd) + '/'\n",
    "    return save, backup, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010000000000000004\n",
      "LeNet5 make_layers: feature cfg [6, 'A', 16, 'A']\n",
      "Weight decay param: parameter name feature.0.conv.weight\n",
      "Weight decay param: parameter name feature.0.batch_norm.weight\n",
      "Weight decay param: parameter name feature.0.batch_norm.bias\n",
      "Weight decay param: parameter name feature.2.conv.weight\n",
      "Weight decay param: parameter name feature.2.batch_norm.weight\n",
      "Weight decay param: parameter name feature.2.batch_norm.bias\n",
      "Weight decay param: parameter name classifier.0.weight\n",
      "Weight decay param: parameter name classifier.0.bias\n",
      "Weight decay param: parameter name classifier.2.weight\n",
      "Weight decay param: parameter name classifier.2.bias\n",
      "Weight decay param: parameter name classifier.4.weight\n",
      "Weight decay param: parameter name classifier.4.bias\n",
      "Start epoch 0/50...\n",
      "Step: 1 Train Epoch: 0 [0/60000 (0.0%)]\tLoss: 2.302664\n",
      "Step: 11 Train Epoch: 0 [2560/60000 (4.3%)]\tLoss: 2.300104\n",
      "Step: 21 Train Epoch: 0 [5120/60000 (8.5%)]\tLoss: 2.302358\n",
      "Step: 31 Train Epoch: 0 [7680/60000 (12.8%)]\tLoss: 2.304058\n",
      "Step: 41 Train Epoch: 0 [10240/60000 (17.0%)]\tLoss: 2.306319\n",
      "Step: 51 Train Epoch: 0 [12800/60000 (21.3%)]\tLoss: 2.304029\n",
      "Step: 61 Train Epoch: 0 [15360/60000 (25.5%)]\tLoss: 2.303587\n",
      "Step: 71 Train Epoch: 0 [17920/60000 (29.8%)]\tLoss: 2.304656\n",
      "Step: 81 Train Epoch: 0 [20480/60000 (34.0%)]\tLoss: 2.300889\n",
      "Step: 91 Train Epoch: 0 [23040/60000 (38.3%)]\tLoss: 2.303773\n",
      "Step: 101 Train Epoch: 0 [25600/60000 (42.6%)]\tLoss: 2.303652\n",
      "Step: 111 Train Epoch: 0 [28160/60000 (46.8%)]\tLoss: 2.301401\n",
      "Step: 121 Train Epoch: 0 [30720/60000 (51.1%)]\tLoss: 2.300905\n",
      "Step: 131 Train Epoch: 0 [33280/60000 (55.3%)]\tLoss: 2.303113\n",
      "Step: 141 Train Epoch: 0 [35840/60000 (59.6%)]\tLoss: 2.301035\n",
      "Step: 151 Train Epoch: 0 [38400/60000 (63.8%)]\tLoss: 2.299430\n",
      "Step: 161 Train Epoch: 0 [40960/60000 (68.1%)]\tLoss: 2.301564\n",
      "Step: 171 Train Epoch: 0 [43520/60000 (72.3%)]\tLoss: 2.307162\n",
      "Step: 181 Train Epoch: 0 [46080/60000 (76.6%)]\tLoss: 2.303965\n",
      "Step: 191 Train Epoch: 0 [48640/60000 (80.9%)]\tLoss: 2.309379\n",
      "Step: 201 Train Epoch: 0 [51200/60000 (85.1%)]\tLoss: 2.308006\n",
      "Step: 211 Train Epoch: 0 [53760/60000 (89.4%)]\tLoss: 2.303574\n",
      "Step: 221 Train Epoch: 0 [56320/60000 (93.6%)]\tLoss: 2.300637\n",
      "Step: 231 Train Epoch: 0 [58880/60000 (97.9%)]\tLoss: 2.302860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laeti\\SHK_NODE\\filter_sparsity\\.venv\\Lib\\site-packages\\torch\\nn\\_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.3034, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 1/50...\n",
      "Step: 236 Train Epoch: 1 [0/60000 (0.0%)]\tLoss: 2.300089\n",
      "Step: 246 Train Epoch: 1 [2560/60000 (4.3%)]\tLoss: 2.300534\n",
      "Step: 256 Train Epoch: 1 [5120/60000 (8.5%)]\tLoss: 2.301837\n",
      "Step: 266 Train Epoch: 1 [7680/60000 (12.8%)]\tLoss: 2.304384\n",
      "Step: 276 Train Epoch: 1 [10240/60000 (17.0%)]\tLoss: 2.302490\n",
      "Step: 286 Train Epoch: 1 [12800/60000 (21.3%)]\tLoss: 2.304910\n",
      "Step: 296 Train Epoch: 1 [15360/60000 (25.5%)]\tLoss: 2.302492\n",
      "Step: 306 Train Epoch: 1 [17920/60000 (29.8%)]\tLoss: 2.304504\n",
      "Step: 316 Train Epoch: 1 [20480/60000 (34.0%)]\tLoss: 2.302083\n",
      "Step: 326 Train Epoch: 1 [23040/60000 (38.3%)]\tLoss: 2.301140\n",
      "Step: 336 Train Epoch: 1 [25600/60000 (42.6%)]\tLoss: 2.302963\n",
      "Step: 346 Train Epoch: 1 [28160/60000 (46.8%)]\tLoss: 2.303276\n",
      "Step: 356 Train Epoch: 1 [30720/60000 (51.1%)]\tLoss: 2.303049\n",
      "Step: 366 Train Epoch: 1 [33280/60000 (55.3%)]\tLoss: 2.303684\n",
      "Step: 376 Train Epoch: 1 [35840/60000 (59.6%)]\tLoss: 2.305434\n",
      "Step: 386 Train Epoch: 1 [38400/60000 (63.8%)]\tLoss: 2.304397\n",
      "Step: 396 Train Epoch: 1 [40960/60000 (68.1%)]\tLoss: 2.305257\n",
      "Step: 406 Train Epoch: 1 [43520/60000 (72.3%)]\tLoss: 2.303599\n",
      "Step: 416 Train Epoch: 1 [46080/60000 (76.6%)]\tLoss: 2.302912\n",
      "Step: 426 Train Epoch: 1 [48640/60000 (80.9%)]\tLoss: 2.301425\n",
      "Step: 436 Train Epoch: 1 [51200/60000 (85.1%)]\tLoss: 2.303212\n",
      "Step: 446 Train Epoch: 1 [53760/60000 (89.4%)]\tLoss: 2.301350\n",
      "Step: 456 Train Epoch: 1 [56320/60000 (93.6%)]\tLoss: 2.301311\n",
      "Step: 466 Train Epoch: 1 [58880/60000 (97.9%)]\tLoss: 2.304319\n",
      "\n",
      "Test set: Average loss: 2.3032, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 2/50...\n",
      "Step: 471 Train Epoch: 2 [0/60000 (0.0%)]\tLoss: 2.306549\n",
      "Step: 481 Train Epoch: 2 [2560/60000 (4.3%)]\tLoss: 2.304319\n",
      "Step: 491 Train Epoch: 2 [5120/60000 (8.5%)]\tLoss: 2.304647\n",
      "Step: 501 Train Epoch: 2 [7680/60000 (12.8%)]\tLoss: 2.301896\n",
      "Step: 511 Train Epoch: 2 [10240/60000 (17.0%)]\tLoss: 2.304119\n",
      "Step: 521 Train Epoch: 2 [12800/60000 (21.3%)]\tLoss: 2.305579\n",
      "Step: 531 Train Epoch: 2 [15360/60000 (25.5%)]\tLoss: 2.300287\n",
      "Step: 541 Train Epoch: 2 [17920/60000 (29.8%)]\tLoss: 2.304365\n",
      "Step: 551 Train Epoch: 2 [20480/60000 (34.0%)]\tLoss: 2.305991\n",
      "Step: 561 Train Epoch: 2 [23040/60000 (38.3%)]\tLoss: 2.303301\n",
      "Step: 571 Train Epoch: 2 [25600/60000 (42.6%)]\tLoss: 2.301784\n",
      "Step: 581 Train Epoch: 2 [28160/60000 (46.8%)]\tLoss: 2.305068\n",
      "Step: 591 Train Epoch: 2 [30720/60000 (51.1%)]\tLoss: 2.302517\n",
      "Step: 601 Train Epoch: 2 [33280/60000 (55.3%)]\tLoss: 2.311487\n",
      "Step: 611 Train Epoch: 2 [35840/60000 (59.6%)]\tLoss: 2.302509\n",
      "Step: 621 Train Epoch: 2 [38400/60000 (63.8%)]\tLoss: 2.306093\n",
      "Step: 631 Train Epoch: 2 [40960/60000 (68.1%)]\tLoss: 2.303062\n",
      "Step: 641 Train Epoch: 2 [43520/60000 (72.3%)]\tLoss: 2.306373\n",
      "Step: 651 Train Epoch: 2 [46080/60000 (76.6%)]\tLoss: 2.305035\n",
      "Step: 661 Train Epoch: 2 [48640/60000 (80.9%)]\tLoss: 2.305347\n",
      "Step: 671 Train Epoch: 2 [51200/60000 (85.1%)]\tLoss: 2.302868\n",
      "Step: 681 Train Epoch: 2 [53760/60000 (89.4%)]\tLoss: 2.303736\n",
      "Step: 691 Train Epoch: 2 [56320/60000 (93.6%)]\tLoss: 2.303404\n",
      "Step: 701 Train Epoch: 2 [58880/60000 (97.9%)]\tLoss: 2.304137\n",
      "\n",
      "Test set: Average loss: 2.3027, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 3/50...\n",
      "Step: 706 Train Epoch: 3 [0/60000 (0.0%)]\tLoss: 2.303420\n",
      "Step: 716 Train Epoch: 3 [2560/60000 (4.3%)]\tLoss: 2.302751\n",
      "Step: 726 Train Epoch: 3 [5120/60000 (8.5%)]\tLoss: 2.302584\n",
      "Step: 736 Train Epoch: 3 [7680/60000 (12.8%)]\tLoss: 2.301675\n",
      "Step: 746 Train Epoch: 3 [10240/60000 (17.0%)]\tLoss: 2.301245\n",
      "Step: 756 Train Epoch: 3 [12800/60000 (21.3%)]\tLoss: 2.303034\n",
      "Step: 766 Train Epoch: 3 [15360/60000 (25.5%)]\tLoss: 2.303340\n",
      "Step: 776 Train Epoch: 3 [17920/60000 (29.8%)]\tLoss: 2.305361\n",
      "Step: 786 Train Epoch: 3 [20480/60000 (34.0%)]\tLoss: 2.302838\n",
      "Step: 796 Train Epoch: 3 [23040/60000 (38.3%)]\tLoss: 2.305858\n",
      "Step: 806 Train Epoch: 3 [25600/60000 (42.6%)]\tLoss: 2.303169\n",
      "Step: 816 Train Epoch: 3 [28160/60000 (46.8%)]\tLoss: 2.306782\n",
      "Step: 826 Train Epoch: 3 [30720/60000 (51.1%)]\tLoss: 2.304699\n",
      "Step: 836 Train Epoch: 3 [33280/60000 (55.3%)]\tLoss: 2.306517\n",
      "Step: 846 Train Epoch: 3 [35840/60000 (59.6%)]\tLoss: 2.305657\n",
      "Step: 856 Train Epoch: 3 [38400/60000 (63.8%)]\tLoss: 2.304852\n",
      "Step: 866 Train Epoch: 3 [40960/60000 (68.1%)]\tLoss: 2.304663\n",
      "Step: 876 Train Epoch: 3 [43520/60000 (72.3%)]\tLoss: 2.303438\n",
      "Step: 886 Train Epoch: 3 [46080/60000 (76.6%)]\tLoss: 2.303159\n",
      "Step: 896 Train Epoch: 3 [48640/60000 (80.9%)]\tLoss: 2.301786\n",
      "Step: 906 Train Epoch: 3 [51200/60000 (85.1%)]\tLoss: 2.303717\n",
      "Step: 916 Train Epoch: 3 [53760/60000 (89.4%)]\tLoss: 2.303066\n",
      "Step: 926 Train Epoch: 3 [56320/60000 (93.6%)]\tLoss: 2.303813\n",
      "Step: 936 Train Epoch: 3 [58880/60000 (97.9%)]\tLoss: 2.303208\n",
      "\n",
      "Test set: Average loss: 2.3027, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 4/50...\n",
      "Step: 941 Train Epoch: 4 [0/60000 (0.0%)]\tLoss: 2.302926\n",
      "Step: 951 Train Epoch: 4 [2560/60000 (4.3%)]\tLoss: 2.302548\n",
      "Step: 961 Train Epoch: 4 [5120/60000 (8.5%)]\tLoss: 2.303975\n",
      "Step: 971 Train Epoch: 4 [7680/60000 (12.8%)]\tLoss: 2.299345\n",
      "Step: 981 Train Epoch: 4 [10240/60000 (17.0%)]\tLoss: 2.302207\n",
      "Step: 991 Train Epoch: 4 [12800/60000 (21.3%)]\tLoss: 2.305254\n",
      "Step: 1001 Train Epoch: 4 [15360/60000 (25.5%)]\tLoss: 2.301006\n",
      "Step: 1011 Train Epoch: 4 [17920/60000 (29.8%)]\tLoss: 2.302155\n",
      "Step: 1021 Train Epoch: 4 [20480/60000 (34.0%)]\tLoss: 2.309379\n",
      "Step: 1031 Train Epoch: 4 [23040/60000 (38.3%)]\tLoss: 2.301387\n",
      "Step: 1041 Train Epoch: 4 [25600/60000 (42.6%)]\tLoss: 2.302986\n",
      "Step: 1051 Train Epoch: 4 [28160/60000 (46.8%)]\tLoss: 2.301994\n",
      "Step: 1061 Train Epoch: 4 [30720/60000 (51.1%)]\tLoss: 2.301000\n",
      "Step: 1071 Train Epoch: 4 [33280/60000 (55.3%)]\tLoss: 2.303830\n",
      "Step: 1081 Train Epoch: 4 [35840/60000 (59.6%)]\tLoss: 2.300781\n",
      "Step: 1091 Train Epoch: 4 [38400/60000 (63.8%)]\tLoss: 2.305102\n",
      "Step: 1101 Train Epoch: 4 [40960/60000 (68.1%)]\tLoss: 2.303047\n",
      "Step: 1111 Train Epoch: 4 [43520/60000 (72.3%)]\tLoss: 2.303422\n",
      "Step: 1121 Train Epoch: 4 [46080/60000 (76.6%)]\tLoss: 2.301916\n",
      "Step: 1131 Train Epoch: 4 [48640/60000 (80.9%)]\tLoss: 2.302824\n",
      "Step: 1141 Train Epoch: 4 [51200/60000 (85.1%)]\tLoss: 2.302701\n",
      "Step: 1151 Train Epoch: 4 [53760/60000 (89.4%)]\tLoss: 2.301508\n",
      "Step: 1161 Train Epoch: 4 [56320/60000 (93.6%)]\tLoss: 2.301178\n",
      "Step: 1171 Train Epoch: 4 [58880/60000 (97.9%)]\tLoss: 2.300432\n",
      "\n",
      "Test set: Average loss: 2.3039, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 5/50...\n",
      "Step: 1176 Train Epoch: 5 [0/60000 (0.0%)]\tLoss: 2.302764\n",
      "Step: 1186 Train Epoch: 5 [2560/60000 (4.3%)]\tLoss: 2.304727\n",
      "Step: 1196 Train Epoch: 5 [5120/60000 (8.5%)]\tLoss: 2.303341\n",
      "Step: 1206 Train Epoch: 5 [7680/60000 (12.8%)]\tLoss: 2.302255\n",
      "Step: 1216 Train Epoch: 5 [10240/60000 (17.0%)]\tLoss: 2.303149\n",
      "Step: 1226 Train Epoch: 5 [12800/60000 (21.3%)]\tLoss: 2.303676\n",
      "Step: 1236 Train Epoch: 5 [15360/60000 (25.5%)]\tLoss: 2.306838\n",
      "Step: 1246 Train Epoch: 5 [17920/60000 (29.8%)]\tLoss: 2.301116\n",
      "Step: 1256 Train Epoch: 5 [20480/60000 (34.0%)]\tLoss: 2.307168\n",
      "Step: 1266 Train Epoch: 5 [23040/60000 (38.3%)]\tLoss: 2.304760\n",
      "Step: 1276 Train Epoch: 5 [25600/60000 (42.6%)]\tLoss: 2.300852\n",
      "Step: 1286 Train Epoch: 5 [28160/60000 (46.8%)]\tLoss: 2.305731\n",
      "Step: 1296 Train Epoch: 5 [30720/60000 (51.1%)]\tLoss: 2.300148\n",
      "Step: 1306 Train Epoch: 5 [33280/60000 (55.3%)]\tLoss: 2.305907\n",
      "Step: 1316 Train Epoch: 5 [35840/60000 (59.6%)]\tLoss: 2.306394\n",
      "Step: 1326 Train Epoch: 5 [38400/60000 (63.8%)]\tLoss: 2.304001\n",
      "Step: 1336 Train Epoch: 5 [40960/60000 (68.1%)]\tLoss: 2.303223\n",
      "Step: 1346 Train Epoch: 5 [43520/60000 (72.3%)]\tLoss: 2.303932\n",
      "Step: 1356 Train Epoch: 5 [46080/60000 (76.6%)]\tLoss: 2.304358\n",
      "Step: 1366 Train Epoch: 5 [48640/60000 (80.9%)]\tLoss: 2.303547\n",
      "Step: 1376 Train Epoch: 5 [51200/60000 (85.1%)]\tLoss: 2.303662\n",
      "Step: 1386 Train Epoch: 5 [53760/60000 (89.4%)]\tLoss: 2.303122\n",
      "Step: 1396 Train Epoch: 5 [56320/60000 (93.6%)]\tLoss: 2.300576\n",
      "Step: 1406 Train Epoch: 5 [58880/60000 (97.9%)]\tLoss: 2.303809\n",
      "\n",
      "Test set: Average loss: 2.3035, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 6/50...\n",
      "Step: 1411 Train Epoch: 6 [0/60000 (0.0%)]\tLoss: 2.307530\n",
      "Step: 1421 Train Epoch: 6 [2560/60000 (4.3%)]\tLoss: 2.304359\n",
      "Step: 1431 Train Epoch: 6 [5120/60000 (8.5%)]\tLoss: 2.305123\n",
      "Step: 1441 Train Epoch: 6 [7680/60000 (12.8%)]\tLoss: 2.301051\n",
      "Step: 1451 Train Epoch: 6 [10240/60000 (17.0%)]\tLoss: 2.303747\n",
      "Step: 1461 Train Epoch: 6 [12800/60000 (21.3%)]\tLoss: 2.304786\n",
      "Step: 1471 Train Epoch: 6 [15360/60000 (25.5%)]\tLoss: 2.303430\n",
      "Step: 1481 Train Epoch: 6 [17920/60000 (29.8%)]\tLoss: 2.304251\n",
      "Step: 1491 Train Epoch: 6 [20480/60000 (34.0%)]\tLoss: 2.301672\n",
      "Step: 1501 Train Epoch: 6 [23040/60000 (38.3%)]\tLoss: 2.303962\n",
      "Step: 1511 Train Epoch: 6 [25600/60000 (42.6%)]\tLoss: 2.301049\n",
      "Step: 1521 Train Epoch: 6 [28160/60000 (46.8%)]\tLoss: 2.303181\n",
      "Step: 1531 Train Epoch: 6 [30720/60000 (51.1%)]\tLoss: 2.303493\n",
      "Step: 1541 Train Epoch: 6 [33280/60000 (55.3%)]\tLoss: 2.302742\n",
      "Step: 1551 Train Epoch: 6 [35840/60000 (59.6%)]\tLoss: 2.303430\n",
      "Step: 1561 Train Epoch: 6 [38400/60000 (63.8%)]\tLoss: 2.302862\n",
      "Step: 1571 Train Epoch: 6 [40960/60000 (68.1%)]\tLoss: 2.306263\n",
      "Step: 1581 Train Epoch: 6 [43520/60000 (72.3%)]\tLoss: 2.299554\n",
      "Step: 1591 Train Epoch: 6 [46080/60000 (76.6%)]\tLoss: 2.303111\n",
      "Step: 1601 Train Epoch: 6 [48640/60000 (80.9%)]\tLoss: 2.305746\n",
      "Step: 1611 Train Epoch: 6 [51200/60000 (85.1%)]\tLoss: 2.304262\n",
      "Step: 1621 Train Epoch: 6 [53760/60000 (89.4%)]\tLoss: 2.309164\n",
      "Step: 1631 Train Epoch: 6 [56320/60000 (93.6%)]\tLoss: 2.306395\n",
      "Step: 1641 Train Epoch: 6 [58880/60000 (97.9%)]\tLoss: 2.300713\n",
      "\n",
      "Test set: Average loss: 2.3042, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 7/50...\n",
      "Step: 1646 Train Epoch: 7 [0/60000 (0.0%)]\tLoss: 2.300521\n",
      "Step: 1656 Train Epoch: 7 [2560/60000 (4.3%)]\tLoss: 2.303861\n",
      "Step: 1666 Train Epoch: 7 [5120/60000 (8.5%)]\tLoss: 2.300162\n",
      "Step: 1676 Train Epoch: 7 [7680/60000 (12.8%)]\tLoss: 2.301241\n",
      "Step: 1686 Train Epoch: 7 [10240/60000 (17.0%)]\tLoss: 2.303262\n",
      "Step: 1696 Train Epoch: 7 [12800/60000 (21.3%)]\tLoss: 2.301055\n",
      "Step: 1706 Train Epoch: 7 [15360/60000 (25.5%)]\tLoss: 2.299265\n",
      "Step: 1716 Train Epoch: 7 [17920/60000 (29.8%)]\tLoss: 2.303384\n",
      "Step: 1726 Train Epoch: 7 [20480/60000 (34.0%)]\tLoss: 2.303486\n",
      "Step: 1736 Train Epoch: 7 [23040/60000 (38.3%)]\tLoss: 2.302065\n",
      "Step: 1746 Train Epoch: 7 [25600/60000 (42.6%)]\tLoss: 2.300571\n",
      "Step: 1756 Train Epoch: 7 [28160/60000 (46.8%)]\tLoss: 2.302737\n",
      "Step: 1766 Train Epoch: 7 [30720/60000 (51.1%)]\tLoss: 2.305904\n",
      "Step: 1776 Train Epoch: 7 [33280/60000 (55.3%)]\tLoss: 2.301898\n",
      "Step: 1786 Train Epoch: 7 [35840/60000 (59.6%)]\tLoss: 2.304458\n",
      "Step: 1796 Train Epoch: 7 [38400/60000 (63.8%)]\tLoss: 2.300189\n",
      "Step: 1806 Train Epoch: 7 [40960/60000 (68.1%)]\tLoss: 2.295860\n",
      "Step: 1816 Train Epoch: 7 [43520/60000 (72.3%)]\tLoss: 2.300792\n",
      "Step: 1826 Train Epoch: 7 [46080/60000 (76.6%)]\tLoss: 2.307852\n",
      "Step: 1836 Train Epoch: 7 [48640/60000 (80.9%)]\tLoss: 2.302369\n",
      "Step: 1846 Train Epoch: 7 [51200/60000 (85.1%)]\tLoss: 2.302796\n",
      "Step: 1856 Train Epoch: 7 [53760/60000 (89.4%)]\tLoss: 2.307591\n",
      "Step: 1866 Train Epoch: 7 [56320/60000 (93.6%)]\tLoss: 2.305394\n",
      "Step: 1876 Train Epoch: 7 [58880/60000 (97.9%)]\tLoss: 2.304550\n",
      "\n",
      "Test set: Average loss: 2.3029, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 8/50...\n",
      "Step: 1881 Train Epoch: 8 [0/60000 (0.0%)]\tLoss: 2.304325\n",
      "Step: 1891 Train Epoch: 8 [2560/60000 (4.3%)]\tLoss: 2.303456\n",
      "Step: 1901 Train Epoch: 8 [5120/60000 (8.5%)]\tLoss: 2.300793\n",
      "Step: 1911 Train Epoch: 8 [7680/60000 (12.8%)]\tLoss: 2.302757\n",
      "Step: 1921 Train Epoch: 8 [10240/60000 (17.0%)]\tLoss: 2.301274\n",
      "Step: 1931 Train Epoch: 8 [12800/60000 (21.3%)]\tLoss: 2.302056\n",
      "Step: 1941 Train Epoch: 8 [15360/60000 (25.5%)]\tLoss: 2.304489\n",
      "Step: 1951 Train Epoch: 8 [17920/60000 (29.8%)]\tLoss: 2.303006\n",
      "Step: 1961 Train Epoch: 8 [20480/60000 (34.0%)]\tLoss: 2.301903\n",
      "Step: 1971 Train Epoch: 8 [23040/60000 (38.3%)]\tLoss: 2.300894\n",
      "Step: 1981 Train Epoch: 8 [25600/60000 (42.6%)]\tLoss: 2.303409\n",
      "Step: 1991 Train Epoch: 8 [28160/60000 (46.8%)]\tLoss: 2.302917\n",
      "Step: 2001 Train Epoch: 8 [30720/60000 (51.1%)]\tLoss: 2.306221\n",
      "Step: 2011 Train Epoch: 8 [33280/60000 (55.3%)]\tLoss: 2.299488\n",
      "Step: 2021 Train Epoch: 8 [35840/60000 (59.6%)]\tLoss: 2.298319\n",
      "Step: 2031 Train Epoch: 8 [38400/60000 (63.8%)]\tLoss: 2.305198\n",
      "Step: 2041 Train Epoch: 8 [40960/60000 (68.1%)]\tLoss: 2.302982\n",
      "Step: 2051 Train Epoch: 8 [43520/60000 (72.3%)]\tLoss: 2.304637\n",
      "Step: 2061 Train Epoch: 8 [46080/60000 (76.6%)]\tLoss: 2.302303\n",
      "Step: 2071 Train Epoch: 8 [48640/60000 (80.9%)]\tLoss: 2.305956\n",
      "Step: 2081 Train Epoch: 8 [51200/60000 (85.1%)]\tLoss: 2.303838\n",
      "Step: 2091 Train Epoch: 8 [53760/60000 (89.4%)]\tLoss: 2.303237\n",
      "Step: 2101 Train Epoch: 8 [56320/60000 (93.6%)]\tLoss: 2.301844\n",
      "Step: 2111 Train Epoch: 8 [58880/60000 (97.9%)]\tLoss: 2.305005\n",
      "\n",
      "Test set: Average loss: 2.3028, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 9/50...\n",
      "Step: 2116 Train Epoch: 9 [0/60000 (0.0%)]\tLoss: 2.302177\n",
      "Step: 2126 Train Epoch: 9 [2560/60000 (4.3%)]\tLoss: 2.301332\n",
      "Step: 2136 Train Epoch: 9 [5120/60000 (8.5%)]\tLoss: 2.301702\n",
      "Step: 2146 Train Epoch: 9 [7680/60000 (12.8%)]\tLoss: 2.306061\n",
      "Step: 2156 Train Epoch: 9 [10240/60000 (17.0%)]\tLoss: 2.300128\n",
      "Step: 2166 Train Epoch: 9 [12800/60000 (21.3%)]\tLoss: 2.302228\n",
      "Step: 2176 Train Epoch: 9 [15360/60000 (25.5%)]\tLoss: 2.301989\n",
      "Step: 2186 Train Epoch: 9 [17920/60000 (29.8%)]\tLoss: 2.302447\n",
      "Step: 2196 Train Epoch: 9 [20480/60000 (34.0%)]\tLoss: 2.302584\n",
      "Step: 2206 Train Epoch: 9 [23040/60000 (38.3%)]\tLoss: 2.305852\n",
      "Step: 2216 Train Epoch: 9 [25600/60000 (42.6%)]\tLoss: 2.297681\n",
      "Step: 2226 Train Epoch: 9 [28160/60000 (46.8%)]\tLoss: 2.309793\n",
      "Step: 2236 Train Epoch: 9 [30720/60000 (51.1%)]\tLoss: 2.303993\n",
      "Step: 2246 Train Epoch: 9 [33280/60000 (55.3%)]\tLoss: 2.305414\n",
      "Step: 2256 Train Epoch: 9 [35840/60000 (59.6%)]\tLoss: 2.298715\n",
      "Step: 2266 Train Epoch: 9 [38400/60000 (63.8%)]\tLoss: 2.303534\n",
      "Step: 2276 Train Epoch: 9 [40960/60000 (68.1%)]\tLoss: 2.301117\n",
      "Step: 2286 Train Epoch: 9 [43520/60000 (72.3%)]\tLoss: 2.306227\n",
      "Step: 2296 Train Epoch: 9 [46080/60000 (76.6%)]\tLoss: 2.305390\n",
      "Step: 2306 Train Epoch: 9 [48640/60000 (80.9%)]\tLoss: 2.302191\n",
      "Step: 2316 Train Epoch: 9 [51200/60000 (85.1%)]\tLoss: 2.303870\n",
      "Step: 2326 Train Epoch: 9 [53760/60000 (89.4%)]\tLoss: 2.302945\n",
      "Step: 2336 Train Epoch: 9 [56320/60000 (93.6%)]\tLoss: 2.303100\n",
      "Step: 2346 Train Epoch: 9 [58880/60000 (97.9%)]\tLoss: 2.302294\n",
      "\n",
      "Test set: Average loss: 2.3029, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 10/50...\n",
      "Step: 2351 Train Epoch: 10 [0/60000 (0.0%)]\tLoss: 2.301470\n",
      "Step: 2361 Train Epoch: 10 [2560/60000 (4.3%)]\tLoss: 2.301703\n",
      "Step: 2371 Train Epoch: 10 [5120/60000 (8.5%)]\tLoss: 2.299755\n",
      "Step: 2381 Train Epoch: 10 [7680/60000 (12.8%)]\tLoss: 2.299160\n",
      "Step: 2391 Train Epoch: 10 [10240/60000 (17.0%)]\tLoss: 2.302543\n",
      "Step: 2401 Train Epoch: 10 [12800/60000 (21.3%)]\tLoss: 2.302234\n",
      "Step: 2411 Train Epoch: 10 [15360/60000 (25.5%)]\tLoss: 2.304059\n",
      "Step: 2421 Train Epoch: 10 [17920/60000 (29.8%)]\tLoss: 2.300638\n",
      "Step: 2431 Train Epoch: 10 [20480/60000 (34.0%)]\tLoss: 2.302603\n",
      "Step: 2441 Train Epoch: 10 [23040/60000 (38.3%)]\tLoss: 2.302024\n",
      "Step: 2451 Train Epoch: 10 [25600/60000 (42.6%)]\tLoss: 2.303568\n",
      "Step: 2461 Train Epoch: 10 [28160/60000 (46.8%)]\tLoss: 2.305310\n",
      "Step: 2471 Train Epoch: 10 [30720/60000 (51.1%)]\tLoss: 2.305220\n",
      "Step: 2481 Train Epoch: 10 [33280/60000 (55.3%)]\tLoss: 2.300531\n",
      "Step: 2491 Train Epoch: 10 [35840/60000 (59.6%)]\tLoss: 2.303949\n",
      "Step: 2501 Train Epoch: 10 [38400/60000 (63.8%)]\tLoss: 2.303374\n",
      "Step: 2511 Train Epoch: 10 [40960/60000 (68.1%)]\tLoss: 2.302375\n",
      "Step: 2521 Train Epoch: 10 [43520/60000 (72.3%)]\tLoss: 2.303061\n",
      "Step: 2531 Train Epoch: 10 [46080/60000 (76.6%)]\tLoss: 2.302912\n",
      "Step: 2541 Train Epoch: 10 [48640/60000 (80.9%)]\tLoss: 2.302016\n",
      "Step: 2551 Train Epoch: 10 [51200/60000 (85.1%)]\tLoss: 2.304029\n",
      "Step: 2561 Train Epoch: 10 [53760/60000 (89.4%)]\tLoss: 2.304553\n",
      "Step: 2571 Train Epoch: 10 [56320/60000 (93.6%)]\tLoss: 2.303374\n",
      "Step: 2581 Train Epoch: 10 [58880/60000 (97.9%)]\tLoss: 2.308547\n",
      "\n",
      "Test set: Average loss: 2.3038, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 11/50...\n",
      "Step: 2586 Train Epoch: 11 [0/60000 (0.0%)]\tLoss: 2.304082\n",
      "Step: 2596 Train Epoch: 11 [2560/60000 (4.3%)]\tLoss: 2.301455\n",
      "Step: 2606 Train Epoch: 11 [5120/60000 (8.5%)]\tLoss: 2.306938\n",
      "Step: 2616 Train Epoch: 11 [7680/60000 (12.8%)]\tLoss: 2.307741\n",
      "Step: 2626 Train Epoch: 11 [10240/60000 (17.0%)]\tLoss: 2.304091\n",
      "Step: 2636 Train Epoch: 11 [12800/60000 (21.3%)]\tLoss: 2.300195\n",
      "Step: 2646 Train Epoch: 11 [15360/60000 (25.5%)]\tLoss: 2.307840\n",
      "Step: 2656 Train Epoch: 11 [17920/60000 (29.8%)]\tLoss: 2.309929\n",
      "Step: 2666 Train Epoch: 11 [20480/60000 (34.0%)]\tLoss: 2.304887\n",
      "Step: 2676 Train Epoch: 11 [23040/60000 (38.3%)]\tLoss: 2.304503\n",
      "Step: 2686 Train Epoch: 11 [25600/60000 (42.6%)]\tLoss: 2.303693\n",
      "Step: 2696 Train Epoch: 11 [28160/60000 (46.8%)]\tLoss: 2.300133\n",
      "Step: 2706 Train Epoch: 11 [30720/60000 (51.1%)]\tLoss: 2.303288\n",
      "Step: 2716 Train Epoch: 11 [33280/60000 (55.3%)]\tLoss: 2.303845\n",
      "Step: 2726 Train Epoch: 11 [35840/60000 (59.6%)]\tLoss: 2.304287\n",
      "Step: 2736 Train Epoch: 11 [38400/60000 (63.8%)]\tLoss: 2.303597\n",
      "Step: 2746 Train Epoch: 11 [40960/60000 (68.1%)]\tLoss: 2.302727\n",
      "Step: 2756 Train Epoch: 11 [43520/60000 (72.3%)]\tLoss: 2.301180\n",
      "Step: 2766 Train Epoch: 11 [46080/60000 (76.6%)]\tLoss: 2.303609\n",
      "Step: 2776 Train Epoch: 11 [48640/60000 (80.9%)]\tLoss: 2.308745\n",
      "Step: 2786 Train Epoch: 11 [51200/60000 (85.1%)]\tLoss: 2.305088\n",
      "Step: 2796 Train Epoch: 11 [53760/60000 (89.4%)]\tLoss: 2.308681\n",
      "Step: 2806 Train Epoch: 11 [56320/60000 (93.6%)]\tLoss: 2.300295\n",
      "Step: 2816 Train Epoch: 11 [58880/60000 (97.9%)]\tLoss: 2.310546\n",
      "\n",
      "Test set: Average loss: 2.3043, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 12/50...\n",
      "Step: 2821 Train Epoch: 12 [0/60000 (0.0%)]\tLoss: 2.300898\n",
      "Step: 2831 Train Epoch: 12 [2560/60000 (4.3%)]\tLoss: 2.305398\n",
      "Step: 2841 Train Epoch: 12 [5120/60000 (8.5%)]\tLoss: 2.299933\n",
      "Step: 2851 Train Epoch: 12 [7680/60000 (12.8%)]\tLoss: 2.304535\n",
      "Step: 2861 Train Epoch: 12 [10240/60000 (17.0%)]\tLoss: 2.305111\n",
      "Step: 2871 Train Epoch: 12 [12800/60000 (21.3%)]\tLoss: 2.304465\n",
      "Step: 2881 Train Epoch: 12 [15360/60000 (25.5%)]\tLoss: 2.306362\n",
      "Step: 2891 Train Epoch: 12 [17920/60000 (29.8%)]\tLoss: 2.303110\n",
      "Step: 2901 Train Epoch: 12 [20480/60000 (34.0%)]\tLoss: 2.300291\n",
      "Step: 2911 Train Epoch: 12 [23040/60000 (38.3%)]\tLoss: 2.299497\n",
      "Step: 2921 Train Epoch: 12 [25600/60000 (42.6%)]\tLoss: 2.302556\n",
      "Step: 2931 Train Epoch: 12 [28160/60000 (46.8%)]\tLoss: 2.304616\n",
      "Step: 2941 Train Epoch: 12 [30720/60000 (51.1%)]\tLoss: 2.303068\n",
      "Step: 2951 Train Epoch: 12 [33280/60000 (55.3%)]\tLoss: 2.302494\n",
      "Step: 2961 Train Epoch: 12 [35840/60000 (59.6%)]\tLoss: 2.303895\n",
      "Step: 2971 Train Epoch: 12 [38400/60000 (63.8%)]\tLoss: 2.301331\n",
      "Step: 2981 Train Epoch: 12 [40960/60000 (68.1%)]\tLoss: 2.302428\n",
      "Step: 2991 Train Epoch: 12 [43520/60000 (72.3%)]\tLoss: 2.300848\n",
      "Step: 3001 Train Epoch: 12 [46080/60000 (76.6%)]\tLoss: 2.299707\n",
      "Step: 3011 Train Epoch: 12 [48640/60000 (80.9%)]\tLoss: 2.304122\n",
      "Step: 3021 Train Epoch: 12 [51200/60000 (85.1%)]\tLoss: 2.300404\n",
      "Step: 3031 Train Epoch: 12 [53760/60000 (89.4%)]\tLoss: 2.303239\n",
      "Step: 3041 Train Epoch: 12 [56320/60000 (93.6%)]\tLoss: 2.305773\n",
      "Step: 3051 Train Epoch: 12 [58880/60000 (97.9%)]\tLoss: 2.306000\n",
      "\n",
      "Test set: Average loss: 2.3039, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 13/50...\n",
      "Step: 3056 Train Epoch: 13 [0/60000 (0.0%)]\tLoss: 2.307930\n",
      "Step: 3066 Train Epoch: 13 [2560/60000 (4.3%)]\tLoss: 2.299549\n",
      "Step: 3076 Train Epoch: 13 [5120/60000 (8.5%)]\tLoss: 2.305442\n",
      "Step: 3086 Train Epoch: 13 [7680/60000 (12.8%)]\tLoss: 2.304205\n",
      "Step: 3096 Train Epoch: 13 [10240/60000 (17.0%)]\tLoss: 2.302741\n",
      "Step: 3106 Train Epoch: 13 [12800/60000 (21.3%)]\tLoss: 2.301840\n",
      "Step: 3116 Train Epoch: 13 [15360/60000 (25.5%)]\tLoss: 2.299549\n",
      "Step: 3126 Train Epoch: 13 [17920/60000 (29.8%)]\tLoss: 2.304378\n",
      "Step: 3136 Train Epoch: 13 [20480/60000 (34.0%)]\tLoss: 2.300499\n",
      "Step: 3146 Train Epoch: 13 [23040/60000 (38.3%)]\tLoss: 2.299948\n",
      "Step: 3156 Train Epoch: 13 [25600/60000 (42.6%)]\tLoss: 2.306323\n",
      "Step: 3166 Train Epoch: 13 [28160/60000 (46.8%)]\tLoss: 2.307612\n",
      "Step: 3176 Train Epoch: 13 [30720/60000 (51.1%)]\tLoss: 2.304379\n",
      "Step: 3186 Train Epoch: 13 [33280/60000 (55.3%)]\tLoss: 2.306618\n",
      "Step: 3196 Train Epoch: 13 [35840/60000 (59.6%)]\tLoss: 2.305171\n",
      "Step: 3206 Train Epoch: 13 [38400/60000 (63.8%)]\tLoss: 2.303778\n",
      "Step: 3216 Train Epoch: 13 [40960/60000 (68.1%)]\tLoss: 2.303164\n",
      "Step: 3226 Train Epoch: 13 [43520/60000 (72.3%)]\tLoss: 2.307420\n",
      "Step: 3236 Train Epoch: 13 [46080/60000 (76.6%)]\tLoss: 2.305988\n",
      "Step: 3246 Train Epoch: 13 [48640/60000 (80.9%)]\tLoss: 2.300813\n",
      "Step: 3256 Train Epoch: 13 [51200/60000 (85.1%)]\tLoss: 2.302412\n",
      "Step: 3266 Train Epoch: 13 [53760/60000 (89.4%)]\tLoss: 2.304474\n",
      "Step: 3276 Train Epoch: 13 [56320/60000 (93.6%)]\tLoss: 2.302494\n",
      "Step: 3286 Train Epoch: 13 [58880/60000 (97.9%)]\tLoss: 2.304639\n",
      "\n",
      "Test set: Average loss: 2.3028, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 14/50...\n",
      "Step: 3291 Train Epoch: 14 [0/60000 (0.0%)]\tLoss: 2.302464\n",
      "Step: 3301 Train Epoch: 14 [2560/60000 (4.3%)]\tLoss: 2.304132\n",
      "Step: 3311 Train Epoch: 14 [5120/60000 (8.5%)]\tLoss: 2.303173\n",
      "Step: 3321 Train Epoch: 14 [7680/60000 (12.8%)]\tLoss: 2.304570\n",
      "Step: 3331 Train Epoch: 14 [10240/60000 (17.0%)]\tLoss: 2.302567\n",
      "Step: 3341 Train Epoch: 14 [12800/60000 (21.3%)]\tLoss: 2.302384\n",
      "Step: 3351 Train Epoch: 14 [15360/60000 (25.5%)]\tLoss: 2.303745\n",
      "Step: 3361 Train Epoch: 14 [17920/60000 (29.8%)]\tLoss: 2.303129\n",
      "Step: 3371 Train Epoch: 14 [20480/60000 (34.0%)]\tLoss: 2.309477\n",
      "Step: 3381 Train Epoch: 14 [23040/60000 (38.3%)]\tLoss: 2.302795\n",
      "Step: 3391 Train Epoch: 14 [25600/60000 (42.6%)]\tLoss: 2.303099\n",
      "Step: 3401 Train Epoch: 14 [28160/60000 (46.8%)]\tLoss: 2.310590\n",
      "Step: 3411 Train Epoch: 14 [30720/60000 (51.1%)]\tLoss: 2.304589\n",
      "Step: 3421 Train Epoch: 14 [33280/60000 (55.3%)]\tLoss: 2.307710\n",
      "Step: 3431 Train Epoch: 14 [35840/60000 (59.6%)]\tLoss: 2.303423\n",
      "Step: 3441 Train Epoch: 14 [38400/60000 (63.8%)]\tLoss: 2.303996\n",
      "Step: 3451 Train Epoch: 14 [40960/60000 (68.1%)]\tLoss: 2.300828\n",
      "Step: 3461 Train Epoch: 14 [43520/60000 (72.3%)]\tLoss: 2.307777\n",
      "Step: 3471 Train Epoch: 14 [46080/60000 (76.6%)]\tLoss: 2.304059\n",
      "Step: 3481 Train Epoch: 14 [48640/60000 (80.9%)]\tLoss: 2.303258\n",
      "Step: 3491 Train Epoch: 14 [51200/60000 (85.1%)]\tLoss: 2.301159\n",
      "Step: 3501 Train Epoch: 14 [53760/60000 (89.4%)]\tLoss: 2.304067\n",
      "Step: 3511 Train Epoch: 14 [56320/60000 (93.6%)]\tLoss: 2.302219\n",
      "Step: 3521 Train Epoch: 14 [58880/60000 (97.9%)]\tLoss: 2.303535\n",
      "\n",
      "Test set: Average loss: 2.3028, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 15/50...\n",
      "Step: 3526 Train Epoch: 15 [0/60000 (0.0%)]\tLoss: 2.304471\n",
      "Step: 3536 Train Epoch: 15 [2560/60000 (4.3%)]\tLoss: 2.301791\n",
      "Step: 3546 Train Epoch: 15 [5120/60000 (8.5%)]\tLoss: 2.303101\n",
      "Step: 3556 Train Epoch: 15 [7680/60000 (12.8%)]\tLoss: 2.303921\n",
      "Step: 3566 Train Epoch: 15 [10240/60000 (17.0%)]\tLoss: 2.302667\n",
      "Step: 3576 Train Epoch: 15 [12800/60000 (21.3%)]\tLoss: 2.310866\n",
      "Step: 3586 Train Epoch: 15 [15360/60000 (25.5%)]\tLoss: 2.301480\n",
      "Step: 3596 Train Epoch: 15 [17920/60000 (29.8%)]\tLoss: 2.306183\n",
      "Step: 3606 Train Epoch: 15 [20480/60000 (34.0%)]\tLoss: 2.303198\n",
      "Step: 3616 Train Epoch: 15 [23040/60000 (38.3%)]\tLoss: 2.307996\n",
      "Step: 3626 Train Epoch: 15 [25600/60000 (42.6%)]\tLoss: 2.307518\n",
      "Step: 3636 Train Epoch: 15 [28160/60000 (46.8%)]\tLoss: 2.301450\n",
      "Step: 3646 Train Epoch: 15 [30720/60000 (51.1%)]\tLoss: 2.303321\n",
      "Step: 3656 Train Epoch: 15 [33280/60000 (55.3%)]\tLoss: 2.307539\n",
      "Step: 3666 Train Epoch: 15 [35840/60000 (59.6%)]\tLoss: 2.306801\n",
      "Step: 3676 Train Epoch: 15 [38400/60000 (63.8%)]\tLoss: 2.305395\n",
      "Step: 3686 Train Epoch: 15 [40960/60000 (68.1%)]\tLoss: 2.303498\n",
      "Step: 3696 Train Epoch: 15 [43520/60000 (72.3%)]\tLoss: 2.301892\n",
      "Step: 3706 Train Epoch: 15 [46080/60000 (76.6%)]\tLoss: 2.302580\n",
      "Step: 3716 Train Epoch: 15 [48640/60000 (80.9%)]\tLoss: 2.302094\n",
      "Step: 3726 Train Epoch: 15 [51200/60000 (85.1%)]\tLoss: 2.302769\n",
      "Step: 3736 Train Epoch: 15 [53760/60000 (89.4%)]\tLoss: 2.301627\n",
      "Step: 3746 Train Epoch: 15 [56320/60000 (93.6%)]\tLoss: 2.303637\n",
      "Step: 3756 Train Epoch: 15 [58880/60000 (97.9%)]\tLoss: 2.301141\n",
      "\n",
      "Test set: Average loss: 2.3027, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 16/50...\n",
      "Step: 3761 Train Epoch: 16 [0/60000 (0.0%)]\tLoss: 2.302732\n",
      "Step: 3771 Train Epoch: 16 [2560/60000 (4.3%)]\tLoss: 2.301603\n",
      "Step: 3781 Train Epoch: 16 [5120/60000 (8.5%)]\tLoss: 2.303651\n",
      "Step: 3791 Train Epoch: 16 [7680/60000 (12.8%)]\tLoss: 2.306634\n",
      "Step: 3801 Train Epoch: 16 [10240/60000 (17.0%)]\tLoss: 2.304372\n",
      "Step: 3811 Train Epoch: 16 [12800/60000 (21.3%)]\tLoss: 2.302836\n",
      "Step: 3821 Train Epoch: 16 [15360/60000 (25.5%)]\tLoss: 2.306626\n",
      "Step: 3831 Train Epoch: 16 [17920/60000 (29.8%)]\tLoss: 2.304874\n",
      "Step: 3841 Train Epoch: 16 [20480/60000 (34.0%)]\tLoss: 2.303797\n",
      "Step: 3851 Train Epoch: 16 [23040/60000 (38.3%)]\tLoss: 2.297474\n",
      "Step: 3861 Train Epoch: 16 [25600/60000 (42.6%)]\tLoss: 2.306045\n",
      "Step: 3871 Train Epoch: 16 [28160/60000 (46.8%)]\tLoss: 2.305001\n",
      "Step: 3881 Train Epoch: 16 [30720/60000 (51.1%)]\tLoss: 2.305818\n",
      "Step: 3891 Train Epoch: 16 [33280/60000 (55.3%)]\tLoss: 2.303336\n",
      "Step: 3901 Train Epoch: 16 [35840/60000 (59.6%)]\tLoss: 2.303948\n",
      "Step: 3911 Train Epoch: 16 [38400/60000 (63.8%)]\tLoss: 2.303684\n",
      "Step: 3921 Train Epoch: 16 [40960/60000 (68.1%)]\tLoss: 2.304202\n",
      "Step: 3931 Train Epoch: 16 [43520/60000 (72.3%)]\tLoss: 2.304960\n",
      "Step: 3941 Train Epoch: 16 [46080/60000 (76.6%)]\tLoss: 2.302945\n",
      "Step: 3951 Train Epoch: 16 [48640/60000 (80.9%)]\tLoss: 2.302429\n",
      "Step: 3961 Train Epoch: 16 [51200/60000 (85.1%)]\tLoss: 2.302489\n",
      "Step: 3971 Train Epoch: 16 [53760/60000 (89.4%)]\tLoss: 2.302444\n",
      "Step: 3981 Train Epoch: 16 [56320/60000 (93.6%)]\tLoss: 2.301954\n",
      "Step: 3991 Train Epoch: 16 [58880/60000 (97.9%)]\tLoss: 2.300096\n",
      "\n",
      "Test set: Average loss: 2.3036, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 17/50...\n",
      "Step: 3996 Train Epoch: 17 [0/60000 (0.0%)]\tLoss: 2.306434\n",
      "Step: 4006 Train Epoch: 17 [2560/60000 (4.3%)]\tLoss: 2.302813\n",
      "Step: 4016 Train Epoch: 17 [5120/60000 (8.5%)]\tLoss: 2.300998\n",
      "Step: 4026 Train Epoch: 17 [7680/60000 (12.8%)]\tLoss: 2.307728\n",
      "Step: 4036 Train Epoch: 17 [10240/60000 (17.0%)]\tLoss: 2.309741\n",
      "Step: 4046 Train Epoch: 17 [12800/60000 (21.3%)]\tLoss: 2.305065\n",
      "Step: 4056 Train Epoch: 17 [15360/60000 (25.5%)]\tLoss: 2.302621\n",
      "Step: 4066 Train Epoch: 17 [17920/60000 (29.8%)]\tLoss: 2.303972\n",
      "Step: 4076 Train Epoch: 17 [20480/60000 (34.0%)]\tLoss: 2.301980\n",
      "Step: 4086 Train Epoch: 17 [23040/60000 (38.3%)]\tLoss: 2.305800\n",
      "Step: 4096 Train Epoch: 17 [25600/60000 (42.6%)]\tLoss: 2.301400\n",
      "Step: 4106 Train Epoch: 17 [28160/60000 (46.8%)]\tLoss: 2.300397\n",
      "Step: 4116 Train Epoch: 17 [30720/60000 (51.1%)]\tLoss: 2.302603\n",
      "Step: 4126 Train Epoch: 17 [33280/60000 (55.3%)]\tLoss: 2.302361\n",
      "Step: 4136 Train Epoch: 17 [35840/60000 (59.6%)]\tLoss: 2.304375\n",
      "Step: 4146 Train Epoch: 17 [38400/60000 (63.8%)]\tLoss: 2.304075\n",
      "Step: 4156 Train Epoch: 17 [40960/60000 (68.1%)]\tLoss: 2.300107\n",
      "Step: 4166 Train Epoch: 17 [43520/60000 (72.3%)]\tLoss: 2.302998\n",
      "Step: 4176 Train Epoch: 17 [46080/60000 (76.6%)]\tLoss: 2.303388\n",
      "Step: 4186 Train Epoch: 17 [48640/60000 (80.9%)]\tLoss: 2.297768\n",
      "Step: 4196 Train Epoch: 17 [51200/60000 (85.1%)]\tLoss: 2.304044\n",
      "Step: 4206 Train Epoch: 17 [53760/60000 (89.4%)]\tLoss: 2.300760\n",
      "Step: 4216 Train Epoch: 17 [56320/60000 (93.6%)]\tLoss: 2.302653\n",
      "Step: 4226 Train Epoch: 17 [58880/60000 (97.9%)]\tLoss: 2.300414\n",
      "\n",
      "Test set: Average loss: 2.3038, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 18/50...\n",
      "Step: 4231 Train Epoch: 18 [0/60000 (0.0%)]\tLoss: 2.303503\n",
      "Step: 4241 Train Epoch: 18 [2560/60000 (4.3%)]\tLoss: 2.305520\n",
      "Step: 4251 Train Epoch: 18 [5120/60000 (8.5%)]\tLoss: 2.303533\n",
      "Step: 4261 Train Epoch: 18 [7680/60000 (12.8%)]\tLoss: 2.303945\n",
      "Step: 4271 Train Epoch: 18 [10240/60000 (17.0%)]\tLoss: 2.303073\n",
      "Step: 4281 Train Epoch: 18 [12800/60000 (21.3%)]\tLoss: 2.301737\n",
      "Step: 4291 Train Epoch: 18 [15360/60000 (25.5%)]\tLoss: 2.303264\n",
      "Step: 4301 Train Epoch: 18 [17920/60000 (29.8%)]\tLoss: 2.302417\n",
      "Step: 4311 Train Epoch: 18 [20480/60000 (34.0%)]\tLoss: 2.302696\n",
      "Step: 4321 Train Epoch: 18 [23040/60000 (38.3%)]\tLoss: 2.302306\n",
      "Step: 4331 Train Epoch: 18 [25600/60000 (42.6%)]\tLoss: 2.303647\n",
      "Step: 4341 Train Epoch: 18 [28160/60000 (46.8%)]\tLoss: 2.302129\n",
      "Step: 4351 Train Epoch: 18 [30720/60000 (51.1%)]\tLoss: 2.303626\n",
      "Step: 4361 Train Epoch: 18 [33280/60000 (55.3%)]\tLoss: 2.303298\n",
      "Step: 4371 Train Epoch: 18 [35840/60000 (59.6%)]\tLoss: 2.305977\n",
      "Step: 4381 Train Epoch: 18 [38400/60000 (63.8%)]\tLoss: 2.305413\n",
      "Step: 4391 Train Epoch: 18 [40960/60000 (68.1%)]\tLoss: 2.307421\n",
      "Step: 4401 Train Epoch: 18 [43520/60000 (72.3%)]\tLoss: 2.304874\n",
      "Step: 4411 Train Epoch: 18 [46080/60000 (76.6%)]\tLoss: 2.299560\n",
      "Step: 4421 Train Epoch: 18 [48640/60000 (80.9%)]\tLoss: 2.299683\n",
      "Step: 4431 Train Epoch: 18 [51200/60000 (85.1%)]\tLoss: 2.304942\n",
      "Step: 4441 Train Epoch: 18 [53760/60000 (89.4%)]\tLoss: 2.304908\n",
      "Step: 4451 Train Epoch: 18 [56320/60000 (93.6%)]\tLoss: 2.300074\n",
      "Step: 4461 Train Epoch: 18 [58880/60000 (97.9%)]\tLoss: 2.303434\n",
      "\n",
      "Test set: Average loss: 2.3031, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 19/50...\n",
      "Step: 4466 Train Epoch: 19 [0/60000 (0.0%)]\tLoss: 2.302434\n",
      "Step: 4476 Train Epoch: 19 [2560/60000 (4.3%)]\tLoss: 2.303290\n",
      "Step: 4486 Train Epoch: 19 [5120/60000 (8.5%)]\tLoss: 2.302408\n",
      "Step: 4496 Train Epoch: 19 [7680/60000 (12.8%)]\tLoss: 2.306493\n",
      "Step: 4506 Train Epoch: 19 [10240/60000 (17.0%)]\tLoss: 2.303298\n",
      "Step: 4516 Train Epoch: 19 [12800/60000 (21.3%)]\tLoss: 2.301610\n",
      "Step: 4526 Train Epoch: 19 [15360/60000 (25.5%)]\tLoss: 2.302962\n",
      "Step: 4536 Train Epoch: 19 [17920/60000 (29.8%)]\tLoss: 2.302988\n",
      "Step: 4546 Train Epoch: 19 [20480/60000 (34.0%)]\tLoss: 2.303361\n",
      "Step: 4556 Train Epoch: 19 [23040/60000 (38.3%)]\tLoss: 2.302845\n",
      "Step: 4566 Train Epoch: 19 [25600/60000 (42.6%)]\tLoss: 2.303873\n",
      "Step: 4576 Train Epoch: 19 [28160/60000 (46.8%)]\tLoss: 2.309828\n",
      "Step: 4586 Train Epoch: 19 [30720/60000 (51.1%)]\tLoss: 2.304961\n",
      "Step: 4596 Train Epoch: 19 [33280/60000 (55.3%)]\tLoss: 2.305530\n",
      "Step: 4606 Train Epoch: 19 [35840/60000 (59.6%)]\tLoss: 2.301578\n",
      "Step: 4616 Train Epoch: 19 [38400/60000 (63.8%)]\tLoss: 2.304987\n",
      "Step: 4626 Train Epoch: 19 [40960/60000 (68.1%)]\tLoss: 2.306135\n",
      "Step: 4636 Train Epoch: 19 [43520/60000 (72.3%)]\tLoss: 2.309673\n",
      "Step: 4646 Train Epoch: 19 [46080/60000 (76.6%)]\tLoss: 2.301757\n",
      "Step: 4656 Train Epoch: 19 [48640/60000 (80.9%)]\tLoss: 2.307106\n",
      "Step: 4666 Train Epoch: 19 [51200/60000 (85.1%)]\tLoss: 2.305738\n",
      "Step: 4676 Train Epoch: 19 [53760/60000 (89.4%)]\tLoss: 2.302942\n",
      "Step: 4686 Train Epoch: 19 [56320/60000 (93.6%)]\tLoss: 2.301294\n",
      "Step: 4696 Train Epoch: 19 [58880/60000 (97.9%)]\tLoss: 2.303094\n",
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 20/50...\n",
      "Step: 4701 Train Epoch: 20 [0/60000 (0.0%)]\tLoss: 2.303188\n",
      "Step: 4711 Train Epoch: 20 [2560/60000 (4.3%)]\tLoss: 2.303057\n",
      "Step: 4721 Train Epoch: 20 [5120/60000 (8.5%)]\tLoss: 2.303339\n",
      "Step: 4731 Train Epoch: 20 [7680/60000 (12.8%)]\tLoss: 2.302787\n",
      "Step: 4741 Train Epoch: 20 [10240/60000 (17.0%)]\tLoss: 2.301364\n",
      "Step: 4751 Train Epoch: 20 [12800/60000 (21.3%)]\tLoss: 2.304908\n",
      "Step: 4761 Train Epoch: 20 [15360/60000 (25.5%)]\tLoss: 2.302676\n",
      "Step: 4771 Train Epoch: 20 [17920/60000 (29.8%)]\tLoss: 2.302808\n",
      "Step: 4781 Train Epoch: 20 [20480/60000 (34.0%)]\tLoss: 2.299174\n",
      "Step: 4791 Train Epoch: 20 [23040/60000 (38.3%)]\tLoss: 2.299405\n",
      "Step: 4801 Train Epoch: 20 [25600/60000 (42.6%)]\tLoss: 2.298493\n",
      "Step: 4811 Train Epoch: 20 [28160/60000 (46.8%)]\tLoss: 2.302981\n",
      "Step: 4821 Train Epoch: 20 [30720/60000 (51.1%)]\tLoss: 2.302880\n",
      "Step: 4831 Train Epoch: 20 [33280/60000 (55.3%)]\tLoss: 2.303205\n",
      "Step: 4841 Train Epoch: 20 [35840/60000 (59.6%)]\tLoss: 2.300765\n",
      "Step: 4851 Train Epoch: 20 [38400/60000 (63.8%)]\tLoss: 2.303410\n",
      "Step: 4861 Train Epoch: 20 [40960/60000 (68.1%)]\tLoss: 2.302349\n",
      "Step: 4871 Train Epoch: 20 [43520/60000 (72.3%)]\tLoss: 2.302290\n",
      "Step: 4881 Train Epoch: 20 [46080/60000 (76.6%)]\tLoss: 2.302627\n",
      "Step: 4891 Train Epoch: 20 [48640/60000 (80.9%)]\tLoss: 2.302649\n",
      "Step: 4901 Train Epoch: 20 [51200/60000 (85.1%)]\tLoss: 2.302832\n",
      "Step: 4911 Train Epoch: 20 [53760/60000 (89.4%)]\tLoss: 2.302835\n",
      "Step: 4921 Train Epoch: 20 [56320/60000 (93.6%)]\tLoss: 2.302158\n",
      "Step: 4931 Train Epoch: 20 [58880/60000 (97.9%)]\tLoss: 2.302168\n",
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 21/50...\n",
      "Step: 4936 Train Epoch: 21 [0/60000 (0.0%)]\tLoss: 2.302181\n",
      "Step: 4946 Train Epoch: 21 [2560/60000 (4.3%)]\tLoss: 2.299798\n",
      "Step: 4956 Train Epoch: 21 [5120/60000 (8.5%)]\tLoss: 2.302351\n",
      "Step: 4966 Train Epoch: 21 [7680/60000 (12.8%)]\tLoss: 2.302495\n",
      "Step: 4976 Train Epoch: 21 [10240/60000 (17.0%)]\tLoss: 2.300338\n",
      "Step: 4986 Train Epoch: 21 [12800/60000 (21.3%)]\tLoss: 2.310078\n",
      "Step: 4996 Train Epoch: 21 [15360/60000 (25.5%)]\tLoss: 2.305857\n",
      "Step: 5006 Train Epoch: 21 [17920/60000 (29.8%)]\tLoss: 2.301133\n",
      "Step: 5016 Train Epoch: 21 [20480/60000 (34.0%)]\tLoss: 2.293853\n",
      "Step: 5026 Train Epoch: 21 [23040/60000 (38.3%)]\tLoss: 2.301179\n",
      "Step: 5036 Train Epoch: 21 [25600/60000 (42.6%)]\tLoss: 2.304407\n",
      "Step: 5046 Train Epoch: 21 [28160/60000 (46.8%)]\tLoss: 2.302778\n",
      "Step: 5056 Train Epoch: 21 [30720/60000 (51.1%)]\tLoss: 2.304305\n",
      "Step: 5066 Train Epoch: 21 [33280/60000 (55.3%)]\tLoss: 2.305072\n",
      "Step: 5076 Train Epoch: 21 [35840/60000 (59.6%)]\tLoss: 2.303541\n",
      "Step: 5086 Train Epoch: 21 [38400/60000 (63.8%)]\tLoss: 2.300611\n",
      "Step: 5096 Train Epoch: 21 [40960/60000 (68.1%)]\tLoss: 2.306936\n",
      "Step: 5106 Train Epoch: 21 [43520/60000 (72.3%)]\tLoss: 2.303536\n",
      "Step: 5116 Train Epoch: 21 [46080/60000 (76.6%)]\tLoss: 2.303807\n",
      "Step: 5126 Train Epoch: 21 [48640/60000 (80.9%)]\tLoss: 2.304446\n",
      "Step: 5136 Train Epoch: 21 [51200/60000 (85.1%)]\tLoss: 2.302200\n",
      "Step: 5146 Train Epoch: 21 [53760/60000 (89.4%)]\tLoss: 2.305225\n",
      "Step: 5156 Train Epoch: 21 [56320/60000 (93.6%)]\tLoss: 2.307092\n",
      "Step: 5166 Train Epoch: 21 [58880/60000 (97.9%)]\tLoss: 2.300383\n",
      "\n",
      "Test set: Average loss: 2.3037, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 22/50...\n",
      "Step: 5171 Train Epoch: 22 [0/60000 (0.0%)]\tLoss: 2.306198\n",
      "Step: 5181 Train Epoch: 22 [2560/60000 (4.3%)]\tLoss: 2.303408\n",
      "Step: 5191 Train Epoch: 22 [5120/60000 (8.5%)]\tLoss: 2.307423\n",
      "Step: 5201 Train Epoch: 22 [7680/60000 (12.8%)]\tLoss: 2.299908\n",
      "Step: 5211 Train Epoch: 22 [10240/60000 (17.0%)]\tLoss: 2.301536\n",
      "Step: 5221 Train Epoch: 22 [12800/60000 (21.3%)]\tLoss: 2.307686\n",
      "Step: 5231 Train Epoch: 22 [15360/60000 (25.5%)]\tLoss: 2.309813\n",
      "Step: 5241 Train Epoch: 22 [17920/60000 (29.8%)]\tLoss: 2.302343\n",
      "Step: 5251 Train Epoch: 22 [20480/60000 (34.0%)]\tLoss: 2.303127\n",
      "Step: 5261 Train Epoch: 22 [23040/60000 (38.3%)]\tLoss: 2.302956\n",
      "Step: 5271 Train Epoch: 22 [25600/60000 (42.6%)]\tLoss: 2.302855\n",
      "Step: 5281 Train Epoch: 22 [28160/60000 (46.8%)]\tLoss: 2.304849\n",
      "Step: 5291 Train Epoch: 22 [30720/60000 (51.1%)]\tLoss: 2.303917\n",
      "Step: 5301 Train Epoch: 22 [33280/60000 (55.3%)]\tLoss: 2.301898\n",
      "Step: 5311 Train Epoch: 22 [35840/60000 (59.6%)]\tLoss: 2.305351\n",
      "Step: 5321 Train Epoch: 22 [38400/60000 (63.8%)]\tLoss: 2.301838\n",
      "Step: 5331 Train Epoch: 22 [40960/60000 (68.1%)]\tLoss: 2.303119\n",
      "Step: 5341 Train Epoch: 22 [43520/60000 (72.3%)]\tLoss: 2.300314\n",
      "Step: 5351 Train Epoch: 22 [46080/60000 (76.6%)]\tLoss: 2.302314\n",
      "Step: 5361 Train Epoch: 22 [48640/60000 (80.9%)]\tLoss: 2.302824\n",
      "Step: 5371 Train Epoch: 22 [51200/60000 (85.1%)]\tLoss: 2.303287\n",
      "Step: 5381 Train Epoch: 22 [53760/60000 (89.4%)]\tLoss: 2.300591\n",
      "Step: 5391 Train Epoch: 22 [56320/60000 (93.6%)]\tLoss: 2.306427\n",
      "Step: 5401 Train Epoch: 22 [58880/60000 (97.9%)]\tLoss: 2.304981\n",
      "\n",
      "Test set: Average loss: 2.3030, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 23/50...\n",
      "Step: 5406 Train Epoch: 23 [0/60000 (0.0%)]\tLoss: 2.300609\n",
      "Step: 5416 Train Epoch: 23 [2560/60000 (4.3%)]\tLoss: 2.302995\n",
      "Step: 5426 Train Epoch: 23 [5120/60000 (8.5%)]\tLoss: 2.303559\n",
      "Step: 5436 Train Epoch: 23 [7680/60000 (12.8%)]\tLoss: 2.304963\n",
      "Step: 5446 Train Epoch: 23 [10240/60000 (17.0%)]\tLoss: 2.302810\n",
      "Step: 5456 Train Epoch: 23 [12800/60000 (21.3%)]\tLoss: 2.303902\n",
      "Step: 5466 Train Epoch: 23 [15360/60000 (25.5%)]\tLoss: 2.304914\n",
      "Step: 5476 Train Epoch: 23 [17920/60000 (29.8%)]\tLoss: 2.302348\n",
      "Step: 5486 Train Epoch: 23 [20480/60000 (34.0%)]\tLoss: 2.300423\n",
      "Step: 5496 Train Epoch: 23 [23040/60000 (38.3%)]\tLoss: 2.302578\n",
      "Step: 5506 Train Epoch: 23 [25600/60000 (42.6%)]\tLoss: 2.303234\n",
      "Step: 5516 Train Epoch: 23 [28160/60000 (46.8%)]\tLoss: 2.303801\n",
      "Step: 5526 Train Epoch: 23 [30720/60000 (51.1%)]\tLoss: 2.301687\n",
      "Step: 5536 Train Epoch: 23 [33280/60000 (55.3%)]\tLoss: 2.302822\n",
      "Step: 5546 Train Epoch: 23 [35840/60000 (59.6%)]\tLoss: 2.306199\n",
      "Step: 5556 Train Epoch: 23 [38400/60000 (63.8%)]\tLoss: 2.303864\n",
      "Step: 5566 Train Epoch: 23 [40960/60000 (68.1%)]\tLoss: 2.305315\n",
      "Step: 5576 Train Epoch: 23 [43520/60000 (72.3%)]\tLoss: 2.301984\n",
      "Step: 5586 Train Epoch: 23 [46080/60000 (76.6%)]\tLoss: 2.300715\n",
      "Step: 5596 Train Epoch: 23 [48640/60000 (80.9%)]\tLoss: 2.302771\n",
      "Step: 5606 Train Epoch: 23 [51200/60000 (85.1%)]\tLoss: 2.302898\n",
      "Step: 5616 Train Epoch: 23 [53760/60000 (89.4%)]\tLoss: 2.307706\n",
      "Step: 5626 Train Epoch: 23 [56320/60000 (93.6%)]\tLoss: 2.306416\n",
      "Step: 5636 Train Epoch: 23 [58880/60000 (97.9%)]\tLoss: 2.303904\n",
      "\n",
      "Test set: Average loss: 2.3037, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 24/50...\n",
      "Step: 5641 Train Epoch: 24 [0/60000 (0.0%)]\tLoss: 2.303959\n",
      "Step: 5651 Train Epoch: 24 [2560/60000 (4.3%)]\tLoss: 2.304364\n",
      "Step: 5661 Train Epoch: 24 [5120/60000 (8.5%)]\tLoss: 2.299268\n",
      "Step: 5671 Train Epoch: 24 [7680/60000 (12.8%)]\tLoss: 2.300144\n",
      "Step: 5681 Train Epoch: 24 [10240/60000 (17.0%)]\tLoss: 2.307197\n",
      "Step: 5691 Train Epoch: 24 [12800/60000 (21.3%)]\tLoss: 2.303549\n",
      "Step: 5701 Train Epoch: 24 [15360/60000 (25.5%)]\tLoss: 2.303102\n",
      "Step: 5711 Train Epoch: 24 [17920/60000 (29.8%)]\tLoss: 2.304687\n",
      "Step: 5721 Train Epoch: 24 [20480/60000 (34.0%)]\tLoss: 2.304289\n",
      "Step: 5731 Train Epoch: 24 [23040/60000 (38.3%)]\tLoss: 2.300924\n",
      "Step: 5741 Train Epoch: 24 [25600/60000 (42.6%)]\tLoss: 2.305716\n",
      "Step: 5751 Train Epoch: 24 [28160/60000 (46.8%)]\tLoss: 2.304634\n",
      "Step: 5761 Train Epoch: 24 [30720/60000 (51.1%)]\tLoss: 2.303910\n",
      "Step: 5771 Train Epoch: 24 [33280/60000 (55.3%)]\tLoss: 2.308533\n",
      "Step: 5781 Train Epoch: 24 [35840/60000 (59.6%)]\tLoss: 2.305471\n",
      "Step: 5791 Train Epoch: 24 [38400/60000 (63.8%)]\tLoss: 2.307604\n",
      "Step: 5801 Train Epoch: 24 [40960/60000 (68.1%)]\tLoss: 2.302477\n",
      "Step: 5811 Train Epoch: 24 [43520/60000 (72.3%)]\tLoss: 2.300614\n",
      "Step: 5821 Train Epoch: 24 [46080/60000 (76.6%)]\tLoss: 2.310779\n",
      "Step: 5831 Train Epoch: 24 [48640/60000 (80.9%)]\tLoss: 2.303690\n",
      "Step: 5841 Train Epoch: 24 [51200/60000 (85.1%)]\tLoss: 2.299522\n",
      "Step: 5851 Train Epoch: 24 [53760/60000 (89.4%)]\tLoss: 2.298117\n",
      "Step: 5861 Train Epoch: 24 [56320/60000 (93.6%)]\tLoss: 2.305501\n",
      "Step: 5871 Train Epoch: 24 [58880/60000 (97.9%)]\tLoss: 2.301713\n",
      "\n",
      "Test set: Average loss: 2.3029, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 25/50...\n",
      "Step: 5876 Train Epoch: 25 [0/60000 (0.0%)]\tLoss: 2.301857\n",
      "Step: 5886 Train Epoch: 25 [2560/60000 (4.3%)]\tLoss: 2.303594\n",
      "Step: 5896 Train Epoch: 25 [5120/60000 (8.5%)]\tLoss: 2.303954\n",
      "Step: 5906 Train Epoch: 25 [7680/60000 (12.8%)]\tLoss: 2.302997\n",
      "Step: 5916 Train Epoch: 25 [10240/60000 (17.0%)]\tLoss: 2.303486\n",
      "Step: 5926 Train Epoch: 25 [12800/60000 (21.3%)]\tLoss: 2.301877\n",
      "Step: 5936 Train Epoch: 25 [15360/60000 (25.5%)]\tLoss: 2.304653\n",
      "Step: 5946 Train Epoch: 25 [17920/60000 (29.8%)]\tLoss: 2.303046\n",
      "Step: 5956 Train Epoch: 25 [20480/60000 (34.0%)]\tLoss: 2.301637\n",
      "Step: 5966 Train Epoch: 25 [23040/60000 (38.3%)]\tLoss: 2.304417\n",
      "Step: 5976 Train Epoch: 25 [25600/60000 (42.6%)]\tLoss: 2.304342\n",
      "Step: 5986 Train Epoch: 25 [28160/60000 (46.8%)]\tLoss: 2.306999\n",
      "Step: 5996 Train Epoch: 25 [30720/60000 (51.1%)]\tLoss: 2.311586\n",
      "Step: 6006 Train Epoch: 25 [33280/60000 (55.3%)]\tLoss: 2.303645\n",
      "Step: 6016 Train Epoch: 25 [35840/60000 (59.6%)]\tLoss: 2.303552\n",
      "Step: 6026 Train Epoch: 25 [38400/60000 (63.8%)]\tLoss: 2.304982\n",
      "Step: 6036 Train Epoch: 25 [40960/60000 (68.1%)]\tLoss: 2.301368\n",
      "Step: 6046 Train Epoch: 25 [43520/60000 (72.3%)]\tLoss: 2.304110\n",
      "Step: 6056 Train Epoch: 25 [46080/60000 (76.6%)]\tLoss: 2.303375\n",
      "Step: 6066 Train Epoch: 25 [48640/60000 (80.9%)]\tLoss: 2.302524\n",
      "Step: 6076 Train Epoch: 25 [51200/60000 (85.1%)]\tLoss: 2.304032\n",
      "Step: 6086 Train Epoch: 25 [53760/60000 (89.4%)]\tLoss: 2.302950\n",
      "Step: 6096 Train Epoch: 25 [56320/60000 (93.6%)]\tLoss: 2.305004\n",
      "Step: 6106 Train Epoch: 25 [58880/60000 (97.9%)]\tLoss: 2.301162\n",
      "\n",
      "Test set: Average loss: 2.3029, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 26/50...\n",
      "Step: 6111 Train Epoch: 26 [0/60000 (0.0%)]\tLoss: 2.302362\n",
      "Step: 6121 Train Epoch: 26 [2560/60000 (4.3%)]\tLoss: 2.304275\n",
      "Step: 6131 Train Epoch: 26 [5120/60000 (8.5%)]\tLoss: 2.304214\n",
      "Step: 6141 Train Epoch: 26 [7680/60000 (12.8%)]\tLoss: 2.303111\n",
      "Step: 6151 Train Epoch: 26 [10240/60000 (17.0%)]\tLoss: 2.302112\n",
      "Step: 6161 Train Epoch: 26 [12800/60000 (21.3%)]\tLoss: 2.303535\n",
      "Step: 6171 Train Epoch: 26 [15360/60000 (25.5%)]\tLoss: 2.303241\n",
      "Step: 6181 Train Epoch: 26 [17920/60000 (29.8%)]\tLoss: 2.300322\n",
      "Step: 6191 Train Epoch: 26 [20480/60000 (34.0%)]\tLoss: 2.308110\n",
      "Step: 6201 Train Epoch: 26 [23040/60000 (38.3%)]\tLoss: 2.302066\n",
      "Step: 6211 Train Epoch: 26 [25600/60000 (42.6%)]\tLoss: 2.295707\n",
      "Step: 6221 Train Epoch: 26 [28160/60000 (46.8%)]\tLoss: 2.300424\n",
      "Step: 6231 Train Epoch: 26 [30720/60000 (51.1%)]\tLoss: 2.308335\n",
      "Step: 6241 Train Epoch: 26 [33280/60000 (55.3%)]\tLoss: 2.302192\n",
      "Step: 6251 Train Epoch: 26 [35840/60000 (59.6%)]\tLoss: 2.301517\n",
      "Step: 6261 Train Epoch: 26 [38400/60000 (63.8%)]\tLoss: 2.304609\n",
      "Step: 6271 Train Epoch: 26 [40960/60000 (68.1%)]\tLoss: 2.301782\n",
      "Step: 6281 Train Epoch: 26 [43520/60000 (72.3%)]\tLoss: 2.303039\n",
      "Step: 6291 Train Epoch: 26 [46080/60000 (76.6%)]\tLoss: 2.303684\n",
      "Step: 6301 Train Epoch: 26 [48640/60000 (80.9%)]\tLoss: 2.301816\n",
      "Step: 6311 Train Epoch: 26 [51200/60000 (85.1%)]\tLoss: 2.304383\n",
      "Step: 6321 Train Epoch: 26 [53760/60000 (89.4%)]\tLoss: 2.302613\n",
      "Step: 6331 Train Epoch: 26 [56320/60000 (93.6%)]\tLoss: 2.304186\n",
      "Step: 6341 Train Epoch: 26 [58880/60000 (97.9%)]\tLoss: 2.302421\n",
      "\n",
      "Test set: Average loss: 2.3028, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 27/50...\n",
      "Step: 6346 Train Epoch: 27 [0/60000 (0.0%)]\tLoss: 2.303954\n",
      "Step: 6356 Train Epoch: 27 [2560/60000 (4.3%)]\tLoss: 2.303698\n",
      "Step: 6366 Train Epoch: 27 [5120/60000 (8.5%)]\tLoss: 2.300930\n",
      "Step: 6376 Train Epoch: 27 [7680/60000 (12.8%)]\tLoss: 2.303723\n",
      "Step: 6386 Train Epoch: 27 [10240/60000 (17.0%)]\tLoss: 2.303257\n",
      "Step: 6396 Train Epoch: 27 [12800/60000 (21.3%)]\tLoss: 2.302366\n",
      "Step: 6406 Train Epoch: 27 [15360/60000 (25.5%)]\tLoss: 2.303046\n",
      "Step: 6416 Train Epoch: 27 [17920/60000 (29.8%)]\tLoss: 2.305758\n",
      "Step: 6426 Train Epoch: 27 [20480/60000 (34.0%)]\tLoss: 2.303576\n",
      "Step: 6436 Train Epoch: 27 [23040/60000 (38.3%)]\tLoss: 2.300765\n",
      "Step: 6446 Train Epoch: 27 [25600/60000 (42.6%)]\tLoss: 2.301656\n",
      "Step: 6456 Train Epoch: 27 [28160/60000 (46.8%)]\tLoss: 2.300924\n",
      "Step: 6466 Train Epoch: 27 [30720/60000 (51.1%)]\tLoss: 2.302175\n",
      "Step: 6476 Train Epoch: 27 [33280/60000 (55.3%)]\tLoss: 2.302991\n",
      "Step: 6486 Train Epoch: 27 [35840/60000 (59.6%)]\tLoss: 2.304403\n",
      "Step: 6496 Train Epoch: 27 [38400/60000 (63.8%)]\tLoss: 2.303675\n",
      "Step: 6506 Train Epoch: 27 [40960/60000 (68.1%)]\tLoss: 2.301606\n",
      "Step: 6516 Train Epoch: 27 [43520/60000 (72.3%)]\tLoss: 2.304549\n",
      "Step: 6526 Train Epoch: 27 [46080/60000 (76.6%)]\tLoss: 2.304390\n",
      "Step: 6536 Train Epoch: 27 [48640/60000 (80.9%)]\tLoss: 2.303607\n",
      "Step: 6546 Train Epoch: 27 [51200/60000 (85.1%)]\tLoss: 2.302540\n",
      "Step: 6556 Train Epoch: 27 [53760/60000 (89.4%)]\tLoss: 2.303203\n",
      "Step: 6566 Train Epoch: 27 [56320/60000 (93.6%)]\tLoss: 2.305590\n",
      "Step: 6576 Train Epoch: 27 [58880/60000 (97.9%)]\tLoss: 2.307094\n",
      "\n",
      "Test set: Average loss: 2.3032, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 28/50...\n",
      "Step: 6581 Train Epoch: 28 [0/60000 (0.0%)]\tLoss: 2.302205\n",
      "Step: 6591 Train Epoch: 28 [2560/60000 (4.3%)]\tLoss: 2.304592\n",
      "Step: 6601 Train Epoch: 28 [5120/60000 (8.5%)]\tLoss: 2.303313\n",
      "Step: 6611 Train Epoch: 28 [7680/60000 (12.8%)]\tLoss: 2.303547\n",
      "Step: 6621 Train Epoch: 28 [10240/60000 (17.0%)]\tLoss: 2.297267\n",
      "Step: 6631 Train Epoch: 28 [12800/60000 (21.3%)]\tLoss: 2.306569\n",
      "Step: 6641 Train Epoch: 28 [15360/60000 (25.5%)]\tLoss: 2.304409\n",
      "Step: 6651 Train Epoch: 28 [17920/60000 (29.8%)]\tLoss: 2.305658\n",
      "Step: 6661 Train Epoch: 28 [20480/60000 (34.0%)]\tLoss: 2.302007\n",
      "Step: 6671 Train Epoch: 28 [23040/60000 (38.3%)]\tLoss: 2.302675\n",
      "Step: 6681 Train Epoch: 28 [25600/60000 (42.6%)]\tLoss: 2.303758\n",
      "Step: 6691 Train Epoch: 28 [28160/60000 (46.8%)]\tLoss: 2.303769\n",
      "Step: 6701 Train Epoch: 28 [30720/60000 (51.1%)]\tLoss: 2.301108\n",
      "Step: 6711 Train Epoch: 28 [33280/60000 (55.3%)]\tLoss: 2.303233\n",
      "Step: 6721 Train Epoch: 28 [35840/60000 (59.6%)]\tLoss: 2.304650\n",
      "Step: 6731 Train Epoch: 28 [38400/60000 (63.8%)]\tLoss: 2.304707\n",
      "Step: 6741 Train Epoch: 28 [40960/60000 (68.1%)]\tLoss: 2.302732\n",
      "Step: 6751 Train Epoch: 28 [43520/60000 (72.3%)]\tLoss: 2.303028\n",
      "Step: 6761 Train Epoch: 28 [46080/60000 (76.6%)]\tLoss: 2.305812\n",
      "Step: 6771 Train Epoch: 28 [48640/60000 (80.9%)]\tLoss: 2.311728\n",
      "Step: 6781 Train Epoch: 28 [51200/60000 (85.1%)]\tLoss: 2.299528\n",
      "Step: 6791 Train Epoch: 28 [53760/60000 (89.4%)]\tLoss: 2.300823\n",
      "Step: 6801 Train Epoch: 28 [56320/60000 (93.6%)]\tLoss: 2.298976\n",
      "Step: 6811 Train Epoch: 28 [58880/60000 (97.9%)]\tLoss: 2.307008\n",
      "\n",
      "Test set: Average loss: 2.3047, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 29/50...\n",
      "Step: 6816 Train Epoch: 29 [0/60000 (0.0%)]\tLoss: 2.311219\n",
      "Step: 6826 Train Epoch: 29 [2560/60000 (4.3%)]\tLoss: 2.305576\n",
      "Step: 6836 Train Epoch: 29 [5120/60000 (8.5%)]\tLoss: 2.300603\n",
      "Step: 6846 Train Epoch: 29 [7680/60000 (12.8%)]\tLoss: 2.301412\n",
      "Step: 6856 Train Epoch: 29 [10240/60000 (17.0%)]\tLoss: 2.303762\n",
      "Step: 6866 Train Epoch: 29 [12800/60000 (21.3%)]\tLoss: 2.304597\n",
      "Step: 6876 Train Epoch: 29 [15360/60000 (25.5%)]\tLoss: 2.301229\n",
      "Step: 6886 Train Epoch: 29 [17920/60000 (29.8%)]\tLoss: 2.303326\n",
      "Step: 6896 Train Epoch: 29 [20480/60000 (34.0%)]\tLoss: 2.302142\n",
      "Step: 6906 Train Epoch: 29 [23040/60000 (38.3%)]\tLoss: 2.303726\n",
      "Step: 6916 Train Epoch: 29 [25600/60000 (42.6%)]\tLoss: 2.304853\n",
      "Step: 6926 Train Epoch: 29 [28160/60000 (46.8%)]\tLoss: 2.302855\n",
      "Step: 6936 Train Epoch: 29 [30720/60000 (51.1%)]\tLoss: 2.304495\n",
      "Step: 6946 Train Epoch: 29 [33280/60000 (55.3%)]\tLoss: 2.305362\n",
      "Step: 6956 Train Epoch: 29 [35840/60000 (59.6%)]\tLoss: 2.300549\n",
      "Step: 6966 Train Epoch: 29 [38400/60000 (63.8%)]\tLoss: 2.303446\n",
      "Step: 6976 Train Epoch: 29 [40960/60000 (68.1%)]\tLoss: 2.307270\n",
      "Step: 6986 Train Epoch: 29 [43520/60000 (72.3%)]\tLoss: 2.298812\n",
      "Step: 6996 Train Epoch: 29 [46080/60000 (76.6%)]\tLoss: 2.304603\n",
      "Step: 7006 Train Epoch: 29 [48640/60000 (80.9%)]\tLoss: 2.297964\n",
      "Step: 7016 Train Epoch: 29 [51200/60000 (85.1%)]\tLoss: 2.304865\n",
      "Step: 7026 Train Epoch: 29 [53760/60000 (89.4%)]\tLoss: 2.303874\n",
      "Step: 7036 Train Epoch: 29 [56320/60000 (93.6%)]\tLoss: 2.303836\n",
      "Step: 7046 Train Epoch: 29 [58880/60000 (97.9%)]\tLoss: 2.305581\n",
      "\n",
      "Test set: Average loss: 2.3030, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 30/50...\n",
      "Step: 7051 Train Epoch: 30 [0/60000 (0.0%)]\tLoss: 2.306285\n",
      "Step: 7061 Train Epoch: 30 [2560/60000 (4.3%)]\tLoss: 2.303532\n",
      "Step: 7071 Train Epoch: 30 [5120/60000 (8.5%)]\tLoss: 2.302656\n",
      "Step: 7081 Train Epoch: 30 [7680/60000 (12.8%)]\tLoss: 2.301496\n",
      "Step: 7091 Train Epoch: 30 [10240/60000 (17.0%)]\tLoss: 2.304525\n",
      "Step: 7101 Train Epoch: 30 [12800/60000 (21.3%)]\tLoss: 2.302763\n",
      "Step: 7111 Train Epoch: 30 [15360/60000 (25.5%)]\tLoss: 2.304300\n",
      "Step: 7121 Train Epoch: 30 [17920/60000 (29.8%)]\tLoss: 2.302094\n",
      "Step: 7131 Train Epoch: 30 [20480/60000 (34.0%)]\tLoss: 2.304642\n",
      "Step: 7141 Train Epoch: 30 [23040/60000 (38.3%)]\tLoss: 2.304151\n",
      "Step: 7151 Train Epoch: 30 [25600/60000 (42.6%)]\tLoss: 2.303379\n",
      "Step: 7161 Train Epoch: 30 [28160/60000 (46.8%)]\tLoss: 2.303904\n",
      "Step: 7171 Train Epoch: 30 [30720/60000 (51.1%)]\tLoss: 2.301856\n",
      "Step: 7181 Train Epoch: 30 [33280/60000 (55.3%)]\tLoss: 2.305313\n",
      "Step: 7191 Train Epoch: 30 [35840/60000 (59.6%)]\tLoss: 2.305705\n",
      "Step: 7201 Train Epoch: 30 [38400/60000 (63.8%)]\tLoss: 2.304380\n",
      "Step: 7211 Train Epoch: 30 [40960/60000 (68.1%)]\tLoss: 2.301305\n",
      "Step: 7221 Train Epoch: 30 [43520/60000 (72.3%)]\tLoss: 2.301497\n",
      "Step: 7231 Train Epoch: 30 [46080/60000 (76.6%)]\tLoss: 2.302137\n",
      "Step: 7241 Train Epoch: 30 [48640/60000 (80.9%)]\tLoss: 2.306376\n",
      "Step: 7251 Train Epoch: 30 [51200/60000 (85.1%)]\tLoss: 2.305247\n",
      "Step: 7261 Train Epoch: 30 [53760/60000 (89.4%)]\tLoss: 2.306469\n",
      "Step: 7271 Train Epoch: 30 [56320/60000 (93.6%)]\tLoss: 2.302500\n",
      "Step: 7281 Train Epoch: 30 [58880/60000 (97.9%)]\tLoss: 2.301698\n",
      "\n",
      "Test set: Average loss: 2.3029, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 31/50...\n",
      "Step: 7286 Train Epoch: 31 [0/60000 (0.0%)]\tLoss: 2.303474\n",
      "Step: 7296 Train Epoch: 31 [2560/60000 (4.3%)]\tLoss: 2.302668\n",
      "Step: 7306 Train Epoch: 31 [5120/60000 (8.5%)]\tLoss: 2.300464\n",
      "Step: 7316 Train Epoch: 31 [7680/60000 (12.8%)]\tLoss: 2.302266\n",
      "Step: 7326 Train Epoch: 31 [10240/60000 (17.0%)]\tLoss: 2.302717\n",
      "Step: 7336 Train Epoch: 31 [12800/60000 (21.3%)]\tLoss: 2.302431\n",
      "Step: 7346 Train Epoch: 31 [15360/60000 (25.5%)]\tLoss: 2.302142\n",
      "Step: 7356 Train Epoch: 31 [17920/60000 (29.8%)]\tLoss: 2.305016\n",
      "Step: 7366 Train Epoch: 31 [20480/60000 (34.0%)]\tLoss: 2.301401\n",
      "Step: 7376 Train Epoch: 31 [23040/60000 (38.3%)]\tLoss: 2.303849\n",
      "Step: 7386 Train Epoch: 31 [25600/60000 (42.6%)]\tLoss: 2.301738\n",
      "Step: 7396 Train Epoch: 31 [28160/60000 (46.8%)]\tLoss: 2.300620\n",
      "Step: 7406 Train Epoch: 31 [30720/60000 (51.1%)]\tLoss: 2.304965\n",
      "Step: 7416 Train Epoch: 31 [33280/60000 (55.3%)]\tLoss: 2.306106\n",
      "Step: 7426 Train Epoch: 31 [35840/60000 (59.6%)]\tLoss: 2.303463\n",
      "Step: 7436 Train Epoch: 31 [38400/60000 (63.8%)]\tLoss: 2.310820\n",
      "Step: 7446 Train Epoch: 31 [40960/60000 (68.1%)]\tLoss: 2.304986\n",
      "Step: 7456 Train Epoch: 31 [43520/60000 (72.3%)]\tLoss: 2.301700\n",
      "Step: 7466 Train Epoch: 31 [46080/60000 (76.6%)]\tLoss: 2.302677\n",
      "Step: 7476 Train Epoch: 31 [48640/60000 (80.9%)]\tLoss: 2.302425\n",
      "Step: 7486 Train Epoch: 31 [51200/60000 (85.1%)]\tLoss: 2.303843\n",
      "Step: 7496 Train Epoch: 31 [53760/60000 (89.4%)]\tLoss: 2.302850\n",
      "Step: 7506 Train Epoch: 31 [56320/60000 (93.6%)]\tLoss: 2.303564\n",
      "Step: 7516 Train Epoch: 31 [58880/60000 (97.9%)]\tLoss: 2.302665\n",
      "\n",
      "Test set: Average loss: 2.3027, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 32/50...\n",
      "Step: 7521 Train Epoch: 32 [0/60000 (0.0%)]\tLoss: 2.302751\n",
      "Step: 7531 Train Epoch: 32 [2560/60000 (4.3%)]\tLoss: 2.302611\n",
      "Step: 7541 Train Epoch: 32 [5120/60000 (8.5%)]\tLoss: 2.303467\n",
      "Step: 7551 Train Epoch: 32 [7680/60000 (12.8%)]\tLoss: 2.301791\n",
      "Step: 7561 Train Epoch: 32 [10240/60000 (17.0%)]\tLoss: 2.305857\n",
      "Step: 7571 Train Epoch: 32 [12800/60000 (21.3%)]\tLoss: 2.306544\n",
      "Step: 7581 Train Epoch: 32 [15360/60000 (25.5%)]\tLoss: 2.302081\n",
      "Step: 7591 Train Epoch: 32 [17920/60000 (29.8%)]\tLoss: 2.302333\n",
      "Step: 7601 Train Epoch: 32 [20480/60000 (34.0%)]\tLoss: 2.296640\n",
      "Step: 7611 Train Epoch: 32 [23040/60000 (38.3%)]\tLoss: 2.303435\n",
      "Step: 7621 Train Epoch: 32 [25600/60000 (42.6%)]\tLoss: 2.307027\n",
      "Step: 7631 Train Epoch: 32 [28160/60000 (46.8%)]\tLoss: 2.301253\n",
      "Step: 7641 Train Epoch: 32 [30720/60000 (51.1%)]\tLoss: 2.302480\n",
      "Step: 7651 Train Epoch: 32 [33280/60000 (55.3%)]\tLoss: 2.301966\n",
      "Step: 7661 Train Epoch: 32 [35840/60000 (59.6%)]\tLoss: 2.303563\n",
      "Step: 7671 Train Epoch: 32 [38400/60000 (63.8%)]\tLoss: 2.306420\n",
      "Step: 7681 Train Epoch: 32 [40960/60000 (68.1%)]\tLoss: 2.297902\n",
      "Step: 7691 Train Epoch: 32 [43520/60000 (72.3%)]\tLoss: 2.300368\n",
      "Step: 7701 Train Epoch: 32 [46080/60000 (76.6%)]\tLoss: 2.303493\n",
      "Step: 7711 Train Epoch: 32 [48640/60000 (80.9%)]\tLoss: 2.303891\n",
      "Step: 7721 Train Epoch: 32 [51200/60000 (85.1%)]\tLoss: 2.303207\n",
      "Step: 7731 Train Epoch: 32 [53760/60000 (89.4%)]\tLoss: 2.302773\n",
      "Step: 7741 Train Epoch: 32 [56320/60000 (93.6%)]\tLoss: 2.302277\n",
      "Step: 7751 Train Epoch: 32 [58880/60000 (97.9%)]\tLoss: 2.306041\n",
      "\n",
      "Test set: Average loss: 2.3031, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 33/50...\n",
      "Step: 7756 Train Epoch: 33 [0/60000 (0.0%)]\tLoss: 2.303567\n",
      "Step: 7766 Train Epoch: 33 [2560/60000 (4.3%)]\tLoss: 2.303551\n",
      "Step: 7776 Train Epoch: 33 [5120/60000 (8.5%)]\tLoss: 2.302129\n",
      "Step: 7786 Train Epoch: 33 [7680/60000 (12.8%)]\tLoss: 2.299866\n",
      "Step: 7796 Train Epoch: 33 [10240/60000 (17.0%)]\tLoss: 2.302042\n",
      "Step: 7806 Train Epoch: 33 [12800/60000 (21.3%)]\tLoss: 2.295646\n",
      "Step: 7816 Train Epoch: 33 [15360/60000 (25.5%)]\tLoss: 2.303439\n",
      "Step: 7826 Train Epoch: 33 [17920/60000 (29.8%)]\tLoss: 2.303183\n",
      "Step: 7836 Train Epoch: 33 [20480/60000 (34.0%)]\tLoss: 2.310217\n",
      "Step: 7846 Train Epoch: 33 [23040/60000 (38.3%)]\tLoss: 2.303782\n",
      "Step: 7856 Train Epoch: 33 [25600/60000 (42.6%)]\tLoss: 2.300369\n",
      "Step: 7866 Train Epoch: 33 [28160/60000 (46.8%)]\tLoss: 2.304802\n",
      "Step: 7876 Train Epoch: 33 [30720/60000 (51.1%)]\tLoss: 2.307177\n",
      "Step: 7886 Train Epoch: 33 [33280/60000 (55.3%)]\tLoss: 2.303313\n",
      "Step: 7896 Train Epoch: 33 [35840/60000 (59.6%)]\tLoss: 2.302205\n",
      "Step: 7906 Train Epoch: 33 [38400/60000 (63.8%)]\tLoss: 2.304885\n",
      "Step: 7916 Train Epoch: 33 [40960/60000 (68.1%)]\tLoss: 2.303535\n",
      "Step: 7926 Train Epoch: 33 [43520/60000 (72.3%)]\tLoss: 2.300656\n",
      "Step: 7936 Train Epoch: 33 [46080/60000 (76.6%)]\tLoss: 2.300287\n",
      "Step: 7946 Train Epoch: 33 [48640/60000 (80.9%)]\tLoss: 2.302455\n",
      "Step: 7956 Train Epoch: 33 [51200/60000 (85.1%)]\tLoss: 2.302034\n",
      "Step: 7966 Train Epoch: 33 [53760/60000 (89.4%)]\tLoss: 2.302669\n",
      "Step: 7976 Train Epoch: 33 [56320/60000 (93.6%)]\tLoss: 2.302324\n",
      "Step: 7986 Train Epoch: 33 [58880/60000 (97.9%)]\tLoss: 2.301124\n",
      "\n",
      "Test set: Average loss: 2.3040, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 34/50...\n",
      "Step: 7991 Train Epoch: 34 [0/60000 (0.0%)]\tLoss: 2.305650\n",
      "Step: 8001 Train Epoch: 34 [2560/60000 (4.3%)]\tLoss: 2.305079\n",
      "Step: 8011 Train Epoch: 34 [5120/60000 (8.5%)]\tLoss: 2.302140\n",
      "Step: 8021 Train Epoch: 34 [7680/60000 (12.8%)]\tLoss: 2.304323\n",
      "Step: 8031 Train Epoch: 34 [10240/60000 (17.0%)]\tLoss: 2.296888\n",
      "Step: 8041 Train Epoch: 34 [12800/60000 (21.3%)]\tLoss: 2.305753\n",
      "Step: 8051 Train Epoch: 34 [15360/60000 (25.5%)]\tLoss: 2.301118\n",
      "Step: 8061 Train Epoch: 34 [17920/60000 (29.8%)]\tLoss: 2.311659\n",
      "Step: 8071 Train Epoch: 34 [20480/60000 (34.0%)]\tLoss: 2.305567\n",
      "Step: 8081 Train Epoch: 34 [23040/60000 (38.3%)]\tLoss: 2.301076\n",
      "Step: 8091 Train Epoch: 34 [25600/60000 (42.6%)]\tLoss: 2.302695\n",
      "Step: 8101 Train Epoch: 34 [28160/60000 (46.8%)]\tLoss: 2.304418\n",
      "Step: 8111 Train Epoch: 34 [30720/60000 (51.1%)]\tLoss: 2.303605\n",
      "Step: 8121 Train Epoch: 34 [33280/60000 (55.3%)]\tLoss: 2.303225\n",
      "Step: 8131 Train Epoch: 34 [35840/60000 (59.6%)]\tLoss: 2.303340\n",
      "Step: 8141 Train Epoch: 34 [38400/60000 (63.8%)]\tLoss: 2.303864\n",
      "Step: 8151 Train Epoch: 34 [40960/60000 (68.1%)]\tLoss: 2.301059\n",
      "Step: 8161 Train Epoch: 34 [43520/60000 (72.3%)]\tLoss: 2.304263\n",
      "Step: 8171 Train Epoch: 34 [46080/60000 (76.6%)]\tLoss: 2.299727\n",
      "Step: 8181 Train Epoch: 34 [48640/60000 (80.9%)]\tLoss: 2.301313\n",
      "Step: 8191 Train Epoch: 34 [51200/60000 (85.1%)]\tLoss: 2.304316\n",
      "Step: 8201 Train Epoch: 34 [53760/60000 (89.4%)]\tLoss: 2.306854\n",
      "Step: 8211 Train Epoch: 34 [56320/60000 (93.6%)]\tLoss: 2.302266\n",
      "Step: 8221 Train Epoch: 34 [58880/60000 (97.9%)]\tLoss: 2.311037\n",
      "\n",
      "Test set: Average loss: 2.3037, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 35/50...\n",
      "Step: 8226 Train Epoch: 35 [0/60000 (0.0%)]\tLoss: 2.305995\n",
      "Step: 8236 Train Epoch: 35 [2560/60000 (4.3%)]\tLoss: 2.302430\n",
      "Step: 8246 Train Epoch: 35 [5120/60000 (8.5%)]\tLoss: 2.304188\n",
      "Step: 8256 Train Epoch: 35 [7680/60000 (12.8%)]\tLoss: 2.303664\n",
      "Step: 8266 Train Epoch: 35 [10240/60000 (17.0%)]\tLoss: 2.298112\n",
      "Step: 8276 Train Epoch: 35 [12800/60000 (21.3%)]\tLoss: 2.303518\n",
      "Step: 8286 Train Epoch: 35 [15360/60000 (25.5%)]\tLoss: 2.306465\n",
      "Step: 8296 Train Epoch: 35 [17920/60000 (29.8%)]\tLoss: 2.302624\n",
      "Step: 8306 Train Epoch: 35 [20480/60000 (34.0%)]\tLoss: 2.303113\n",
      "Step: 8316 Train Epoch: 35 [23040/60000 (38.3%)]\tLoss: 2.304710\n",
      "Step: 8326 Train Epoch: 35 [25600/60000 (42.6%)]\tLoss: 2.303127\n",
      "Step: 8336 Train Epoch: 35 [28160/60000 (46.8%)]\tLoss: 2.303052\n",
      "Step: 8346 Train Epoch: 35 [30720/60000 (51.1%)]\tLoss: 2.303437\n",
      "Step: 8356 Train Epoch: 35 [33280/60000 (55.3%)]\tLoss: 2.304713\n",
      "Step: 8366 Train Epoch: 35 [35840/60000 (59.6%)]\tLoss: 2.301480\n",
      "Step: 8376 Train Epoch: 35 [38400/60000 (63.8%)]\tLoss: 2.307405\n",
      "Step: 8386 Train Epoch: 35 [40960/60000 (68.1%)]\tLoss: 2.302289\n",
      "Step: 8396 Train Epoch: 35 [43520/60000 (72.3%)]\tLoss: 2.306237\n",
      "Step: 8406 Train Epoch: 35 [46080/60000 (76.6%)]\tLoss: 2.303083\n",
      "Step: 8416 Train Epoch: 35 [48640/60000 (80.9%)]\tLoss: 2.308744\n",
      "Step: 8426 Train Epoch: 35 [51200/60000 (85.1%)]\tLoss: 2.304917\n",
      "Step: 8436 Train Epoch: 35 [53760/60000 (89.4%)]\tLoss: 2.298612\n",
      "Step: 8446 Train Epoch: 35 [56320/60000 (93.6%)]\tLoss: 2.309177\n",
      "Step: 8456 Train Epoch: 35 [58880/60000 (97.9%)]\tLoss: 2.306153\n",
      "\n",
      "Test set: Average loss: 2.3030, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 36/50...\n",
      "Step: 8461 Train Epoch: 36 [0/60000 (0.0%)]\tLoss: 2.303000\n",
      "Step: 8471 Train Epoch: 36 [2560/60000 (4.3%)]\tLoss: 2.298934\n",
      "Step: 8481 Train Epoch: 36 [5120/60000 (8.5%)]\tLoss: 2.301274\n",
      "Step: 8491 Train Epoch: 36 [7680/60000 (12.8%)]\tLoss: 2.295995\n",
      "Step: 8501 Train Epoch: 36 [10240/60000 (17.0%)]\tLoss: 2.301319\n",
      "Step: 8511 Train Epoch: 36 [12800/60000 (21.3%)]\tLoss: 2.305738\n",
      "Step: 8521 Train Epoch: 36 [15360/60000 (25.5%)]\tLoss: 2.302062\n",
      "Step: 8531 Train Epoch: 36 [17920/60000 (29.8%)]\tLoss: 2.303631\n",
      "Step: 8541 Train Epoch: 36 [20480/60000 (34.0%)]\tLoss: 2.305691\n",
      "Step: 8551 Train Epoch: 36 [23040/60000 (38.3%)]\tLoss: 2.303160\n",
      "Step: 8561 Train Epoch: 36 [25600/60000 (42.6%)]\tLoss: 2.306853\n",
      "Step: 8571 Train Epoch: 36 [28160/60000 (46.8%)]\tLoss: 2.302834\n",
      "Step: 8581 Train Epoch: 36 [30720/60000 (51.1%)]\tLoss: 2.304680\n",
      "Step: 8591 Train Epoch: 36 [33280/60000 (55.3%)]\tLoss: 2.305079\n",
      "Step: 8601 Train Epoch: 36 [35840/60000 (59.6%)]\tLoss: 2.300614\n",
      "Step: 8611 Train Epoch: 36 [38400/60000 (63.8%)]\tLoss: 2.299694\n",
      "Step: 8621 Train Epoch: 36 [40960/60000 (68.1%)]\tLoss: 2.306236\n",
      "Step: 8631 Train Epoch: 36 [43520/60000 (72.3%)]\tLoss: 2.301831\n",
      "Step: 8641 Train Epoch: 36 [46080/60000 (76.6%)]\tLoss: 2.303722\n",
      "Step: 8651 Train Epoch: 36 [48640/60000 (80.9%)]\tLoss: 2.300830\n",
      "Step: 8661 Train Epoch: 36 [51200/60000 (85.1%)]\tLoss: 2.305242\n",
      "Step: 8671 Train Epoch: 36 [53760/60000 (89.4%)]\tLoss: 2.305354\n",
      "Step: 8681 Train Epoch: 36 [56320/60000 (93.6%)]\tLoss: 2.303228\n",
      "Step: 8691 Train Epoch: 36 [58880/60000 (97.9%)]\tLoss: 2.303725\n",
      "\n",
      "Test set: Average loss: 2.3029, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 37/50...\n",
      "Step: 8696 Train Epoch: 37 [0/60000 (0.0%)]\tLoss: 2.300961\n",
      "Step: 8706 Train Epoch: 37 [2560/60000 (4.3%)]\tLoss: 2.300708\n",
      "Step: 8716 Train Epoch: 37 [5120/60000 (8.5%)]\tLoss: 2.301326\n",
      "Step: 8726 Train Epoch: 37 [7680/60000 (12.8%)]\tLoss: 2.301367\n",
      "Step: 8736 Train Epoch: 37 [10240/60000 (17.0%)]\tLoss: 2.301548\n",
      "Step: 8746 Train Epoch: 37 [12800/60000 (21.3%)]\tLoss: 2.302187\n",
      "Step: 8756 Train Epoch: 37 [15360/60000 (25.5%)]\tLoss: 2.297423\n",
      "Step: 8766 Train Epoch: 37 [17920/60000 (29.8%)]\tLoss: 2.302207\n",
      "Step: 8776 Train Epoch: 37 [20480/60000 (34.0%)]\tLoss: 2.307385\n",
      "Step: 8786 Train Epoch: 37 [23040/60000 (38.3%)]\tLoss: 2.303836\n",
      "Step: 8796 Train Epoch: 37 [25600/60000 (42.6%)]\tLoss: 2.303098\n",
      "Step: 8806 Train Epoch: 37 [28160/60000 (46.8%)]\tLoss: 2.311294\n",
      "Step: 8816 Train Epoch: 37 [30720/60000 (51.1%)]\tLoss: 2.301870\n",
      "Step: 8826 Train Epoch: 37 [33280/60000 (55.3%)]\tLoss: 2.303102\n",
      "Step: 8836 Train Epoch: 37 [35840/60000 (59.6%)]\tLoss: 2.308958\n",
      "Step: 8846 Train Epoch: 37 [38400/60000 (63.8%)]\tLoss: 2.301949\n",
      "Step: 8856 Train Epoch: 37 [40960/60000 (68.1%)]\tLoss: 2.304652\n",
      "Step: 8866 Train Epoch: 37 [43520/60000 (72.3%)]\tLoss: 2.303762\n",
      "Step: 8876 Train Epoch: 37 [46080/60000 (76.6%)]\tLoss: 2.306641\n",
      "Step: 8886 Train Epoch: 37 [48640/60000 (80.9%)]\tLoss: 2.303653\n",
      "Step: 8896 Train Epoch: 37 [51200/60000 (85.1%)]\tLoss: 2.304095\n",
      "Step: 8906 Train Epoch: 37 [53760/60000 (89.4%)]\tLoss: 2.302956\n",
      "Step: 8916 Train Epoch: 37 [56320/60000 (93.6%)]\tLoss: 2.302357\n",
      "Step: 8926 Train Epoch: 37 [58880/60000 (97.9%)]\tLoss: 2.303360\n",
      "\n",
      "Test set: Average loss: 2.3028, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 38/50...\n",
      "Step: 8931 Train Epoch: 38 [0/60000 (0.0%)]\tLoss: 2.302417\n",
      "Step: 8941 Train Epoch: 38 [2560/60000 (4.3%)]\tLoss: 2.303433\n",
      "Step: 8951 Train Epoch: 38 [5120/60000 (8.5%)]\tLoss: 2.303302\n",
      "Step: 8961 Train Epoch: 38 [7680/60000 (12.8%)]\tLoss: 2.303657\n",
      "Step: 8971 Train Epoch: 38 [10240/60000 (17.0%)]\tLoss: 2.302348\n",
      "Step: 8981 Train Epoch: 38 [12800/60000 (21.3%)]\tLoss: 2.301766\n",
      "Step: 8991 Train Epoch: 38 [15360/60000 (25.5%)]\tLoss: 2.299559\n",
      "Step: 9001 Train Epoch: 38 [17920/60000 (29.8%)]\tLoss: 2.305138\n",
      "Step: 9011 Train Epoch: 38 [20480/60000 (34.0%)]\tLoss: 2.302233\n",
      "Step: 9021 Train Epoch: 38 [23040/60000 (38.3%)]\tLoss: 2.305078\n",
      "Step: 9031 Train Epoch: 38 [25600/60000 (42.6%)]\tLoss: 2.306817\n",
      "Step: 9041 Train Epoch: 38 [28160/60000 (46.8%)]\tLoss: 2.301979\n",
      "Step: 9051 Train Epoch: 38 [30720/60000 (51.1%)]\tLoss: 2.304115\n",
      "Step: 9061 Train Epoch: 38 [33280/60000 (55.3%)]\tLoss: 2.308614\n",
      "Step: 9071 Train Epoch: 38 [35840/60000 (59.6%)]\tLoss: 2.304416\n",
      "Step: 9081 Train Epoch: 38 [38400/60000 (63.8%)]\tLoss: 2.301447\n",
      "Step: 9091 Train Epoch: 38 [40960/60000 (68.1%)]\tLoss: 2.302818\n",
      "Step: 9101 Train Epoch: 38 [43520/60000 (72.3%)]\tLoss: 2.304396\n",
      "Step: 9111 Train Epoch: 38 [46080/60000 (76.6%)]\tLoss: 2.303559\n",
      "Step: 9121 Train Epoch: 38 [48640/60000 (80.9%)]\tLoss: 2.303534\n",
      "Step: 9131 Train Epoch: 38 [51200/60000 (85.1%)]\tLoss: 2.301656\n",
      "Step: 9141 Train Epoch: 38 [53760/60000 (89.4%)]\tLoss: 2.304069\n",
      "Step: 9151 Train Epoch: 38 [56320/60000 (93.6%)]\tLoss: 2.303618\n",
      "Step: 9161 Train Epoch: 38 [58880/60000 (97.9%)]\tLoss: 2.305781\n",
      "\n",
      "Test set: Average loss: 2.3033, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 39/50...\n",
      "Step: 9166 Train Epoch: 39 [0/60000 (0.0%)]\tLoss: 2.302848\n",
      "Step: 9176 Train Epoch: 39 [2560/60000 (4.3%)]\tLoss: 2.303955\n",
      "Step: 9186 Train Epoch: 39 [5120/60000 (8.5%)]\tLoss: 2.301870\n",
      "Step: 9196 Train Epoch: 39 [7680/60000 (12.8%)]\tLoss: 2.302148\n",
      "Step: 9206 Train Epoch: 39 [10240/60000 (17.0%)]\tLoss: 2.300119\n",
      "Step: 9216 Train Epoch: 39 [12800/60000 (21.3%)]\tLoss: 2.307833\n",
      "Step: 9226 Train Epoch: 39 [15360/60000 (25.5%)]\tLoss: 2.302691\n",
      "Step: 9236 Train Epoch: 39 [17920/60000 (29.8%)]\tLoss: 2.303921\n",
      "Step: 9246 Train Epoch: 39 [20480/60000 (34.0%)]\tLoss: 2.304457\n",
      "Step: 9256 Train Epoch: 39 [23040/60000 (38.3%)]\tLoss: 2.303519\n",
      "Step: 9266 Train Epoch: 39 [25600/60000 (42.6%)]\tLoss: 2.301569\n",
      "Step: 9276 Train Epoch: 39 [28160/60000 (46.8%)]\tLoss: 2.302355\n",
      "Step: 9286 Train Epoch: 39 [30720/60000 (51.1%)]\tLoss: 2.303704\n",
      "Step: 9296 Train Epoch: 39 [33280/60000 (55.3%)]\tLoss: 2.303167\n",
      "Step: 9306 Train Epoch: 39 [35840/60000 (59.6%)]\tLoss: 2.303097\n",
      "Step: 9316 Train Epoch: 39 [38400/60000 (63.8%)]\tLoss: 2.300814\n",
      "Step: 9326 Train Epoch: 39 [40960/60000 (68.1%)]\tLoss: 2.302597\n",
      "Step: 9336 Train Epoch: 39 [43520/60000 (72.3%)]\tLoss: 2.300757\n",
      "Step: 9346 Train Epoch: 39 [46080/60000 (76.6%)]\tLoss: 2.306204\n",
      "Step: 9356 Train Epoch: 39 [48640/60000 (80.9%)]\tLoss: 2.300836\n",
      "Step: 9366 Train Epoch: 39 [51200/60000 (85.1%)]\tLoss: 2.301403\n",
      "Step: 9376 Train Epoch: 39 [53760/60000 (89.4%)]\tLoss: 2.301091\n",
      "Step: 9386 Train Epoch: 39 [56320/60000 (93.6%)]\tLoss: 2.301888\n",
      "Step: 9396 Train Epoch: 39 [58880/60000 (97.9%)]\tLoss: 2.299042\n",
      "\n",
      "Test set: Average loss: 2.3044, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 40/50...\n",
      "Step: 9401 Train Epoch: 40 [0/60000 (0.0%)]\tLoss: 2.306665\n",
      "Step: 9411 Train Epoch: 40 [2560/60000 (4.3%)]\tLoss: 2.309113\n",
      "Step: 9421 Train Epoch: 40 [5120/60000 (8.5%)]\tLoss: 2.305908\n",
      "Step: 9431 Train Epoch: 40 [7680/60000 (12.8%)]\tLoss: 2.306253\n",
      "Step: 9441 Train Epoch: 40 [10240/60000 (17.0%)]\tLoss: 2.305764\n",
      "Step: 9451 Train Epoch: 40 [12800/60000 (21.3%)]\tLoss: 2.305635\n",
      "Step: 9461 Train Epoch: 40 [15360/60000 (25.5%)]\tLoss: 2.300475\n",
      "Step: 9471 Train Epoch: 40 [17920/60000 (29.8%)]\tLoss: 2.302370\n",
      "Step: 9481 Train Epoch: 40 [20480/60000 (34.0%)]\tLoss: 2.303602\n",
      "Step: 9491 Train Epoch: 40 [23040/60000 (38.3%)]\tLoss: 2.305677\n",
      "Step: 9501 Train Epoch: 40 [25600/60000 (42.6%)]\tLoss: 2.304437\n",
      "Step: 9511 Train Epoch: 40 [28160/60000 (46.8%)]\tLoss: 2.303648\n",
      "Step: 9521 Train Epoch: 40 [30720/60000 (51.1%)]\tLoss: 2.303571\n",
      "Step: 9531 Train Epoch: 40 [33280/60000 (55.3%)]\tLoss: 2.302972\n",
      "Step: 9541 Train Epoch: 40 [35840/60000 (59.6%)]\tLoss: 2.303921\n",
      "Step: 9551 Train Epoch: 40 [38400/60000 (63.8%)]\tLoss: 2.303682\n",
      "Step: 9561 Train Epoch: 40 [40960/60000 (68.1%)]\tLoss: 2.302571\n",
      "Step: 9571 Train Epoch: 40 [43520/60000 (72.3%)]\tLoss: 2.303468\n",
      "Step: 9581 Train Epoch: 40 [46080/60000 (76.6%)]\tLoss: 2.299558\n",
      "Step: 9591 Train Epoch: 40 [48640/60000 (80.9%)]\tLoss: 2.309413\n",
      "Step: 9601 Train Epoch: 40 [51200/60000 (85.1%)]\tLoss: 2.303735\n",
      "Step: 9611 Train Epoch: 40 [53760/60000 (89.4%)]\tLoss: 2.302883\n",
      "Step: 9621 Train Epoch: 40 [56320/60000 (93.6%)]\tLoss: 2.306276\n",
      "Step: 9631 Train Epoch: 40 [58880/60000 (97.9%)]\tLoss: 2.305768\n",
      "\n",
      "Test set: Average loss: 2.3033, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 41/50...\n",
      "Step: 9636 Train Epoch: 41 [0/60000 (0.0%)]\tLoss: 2.308559\n",
      "Step: 9646 Train Epoch: 41 [2560/60000 (4.3%)]\tLoss: 2.301666\n",
      "Step: 9656 Train Epoch: 41 [5120/60000 (8.5%)]\tLoss: 2.303391\n",
      "Step: 9666 Train Epoch: 41 [7680/60000 (12.8%)]\tLoss: 2.300708\n",
      "Step: 9676 Train Epoch: 41 [10240/60000 (17.0%)]\tLoss: 2.303752\n",
      "Step: 9686 Train Epoch: 41 [12800/60000 (21.3%)]\tLoss: 2.302901\n",
      "Step: 9696 Train Epoch: 41 [15360/60000 (25.5%)]\tLoss: 2.301691\n",
      "Step: 9706 Train Epoch: 41 [17920/60000 (29.8%)]\tLoss: 2.303682\n",
      "Step: 9716 Train Epoch: 41 [20480/60000 (34.0%)]\tLoss: 2.303725\n",
      "Step: 9726 Train Epoch: 41 [23040/60000 (38.3%)]\tLoss: 2.300930\n",
      "Step: 9736 Train Epoch: 41 [25600/60000 (42.6%)]\tLoss: 2.303501\n",
      "Step: 9746 Train Epoch: 41 [28160/60000 (46.8%)]\tLoss: 2.302266\n",
      "Step: 9756 Train Epoch: 41 [30720/60000 (51.1%)]\tLoss: 2.304891\n",
      "Step: 9766 Train Epoch: 41 [33280/60000 (55.3%)]\tLoss: 2.305013\n",
      "Step: 9776 Train Epoch: 41 [35840/60000 (59.6%)]\tLoss: 2.301913\n",
      "Step: 9786 Train Epoch: 41 [38400/60000 (63.8%)]\tLoss: 2.301889\n",
      "Step: 9796 Train Epoch: 41 [40960/60000 (68.1%)]\tLoss: 2.303721\n",
      "Step: 9806 Train Epoch: 41 [43520/60000 (72.3%)]\tLoss: 2.304358\n",
      "Step: 9816 Train Epoch: 41 [46080/60000 (76.6%)]\tLoss: 2.310332\n",
      "Step: 9826 Train Epoch: 41 [48640/60000 (80.9%)]\tLoss: 2.306103\n",
      "Step: 9836 Train Epoch: 41 [51200/60000 (85.1%)]\tLoss: 2.301525\n",
      "Step: 9846 Train Epoch: 41 [53760/60000 (89.4%)]\tLoss: 2.302247\n",
      "Step: 9856 Train Epoch: 41 [56320/60000 (93.6%)]\tLoss: 2.302654\n",
      "Step: 9866 Train Epoch: 41 [58880/60000 (97.9%)]\tLoss: 2.306331\n",
      "\n",
      "Test set: Average loss: 2.3032, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 42/50...\n",
      "Step: 9871 Train Epoch: 42 [0/60000 (0.0%)]\tLoss: 2.303936\n",
      "Step: 9881 Train Epoch: 42 [2560/60000 (4.3%)]\tLoss: 2.308035\n",
      "Step: 9891 Train Epoch: 42 [5120/60000 (8.5%)]\tLoss: 2.304180\n",
      "Step: 9901 Train Epoch: 42 [7680/60000 (12.8%)]\tLoss: 2.304827\n",
      "Step: 9911 Train Epoch: 42 [10240/60000 (17.0%)]\tLoss: 2.304988\n",
      "Step: 9921 Train Epoch: 42 [12800/60000 (21.3%)]\tLoss: 2.304826\n",
      "Step: 9931 Train Epoch: 42 [15360/60000 (25.5%)]\tLoss: 2.303031\n",
      "Step: 9941 Train Epoch: 42 [17920/60000 (29.8%)]\tLoss: 2.303433\n",
      "Step: 9951 Train Epoch: 42 [20480/60000 (34.0%)]\tLoss: 2.301685\n",
      "Step: 9961 Train Epoch: 42 [23040/60000 (38.3%)]\tLoss: 2.299209\n",
      "Step: 9971 Train Epoch: 42 [25600/60000 (42.6%)]\tLoss: 2.307055\n",
      "Step: 9981 Train Epoch: 42 [28160/60000 (46.8%)]\tLoss: 2.299232\n",
      "Step: 9991 Train Epoch: 42 [30720/60000 (51.1%)]\tLoss: 2.294551\n",
      "Step: 10001 Train Epoch: 42 [33280/60000 (55.3%)]\tLoss: 2.300102\n",
      "Step: 10011 Train Epoch: 42 [35840/60000 (59.6%)]\tLoss: 2.301172\n",
      "Step: 10021 Train Epoch: 42 [38400/60000 (63.8%)]\tLoss: 2.304374\n",
      "Step: 10031 Train Epoch: 42 [40960/60000 (68.1%)]\tLoss: 2.297347\n",
      "Step: 10041 Train Epoch: 42 [43520/60000 (72.3%)]\tLoss: 2.304774\n",
      "Step: 10051 Train Epoch: 42 [46080/60000 (76.6%)]\tLoss: 2.306738\n",
      "Step: 10061 Train Epoch: 42 [48640/60000 (80.9%)]\tLoss: 2.303210\n",
      "Step: 10071 Train Epoch: 42 [51200/60000 (85.1%)]\tLoss: 2.305565\n",
      "Step: 10081 Train Epoch: 42 [53760/60000 (89.4%)]\tLoss: 2.301026\n",
      "Step: 10091 Train Epoch: 42 [56320/60000 (93.6%)]\tLoss: 2.299754\n",
      "Step: 10101 Train Epoch: 42 [58880/60000 (97.9%)]\tLoss: 2.302419\n",
      "\n",
      "Test set: Average loss: 2.3031, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 43/50...\n",
      "Step: 10106 Train Epoch: 43 [0/60000 (0.0%)]\tLoss: 2.305159\n",
      "Step: 10116 Train Epoch: 43 [2560/60000 (4.3%)]\tLoss: 2.304381\n",
      "Step: 10126 Train Epoch: 43 [5120/60000 (8.5%)]\tLoss: 2.303452\n",
      "Step: 10136 Train Epoch: 43 [7680/60000 (12.8%)]\tLoss: 2.304492\n",
      "Step: 10146 Train Epoch: 43 [10240/60000 (17.0%)]\tLoss: 2.304100\n",
      "Step: 10156 Train Epoch: 43 [12800/60000 (21.3%)]\tLoss: 2.300511\n",
      "Step: 10166 Train Epoch: 43 [15360/60000 (25.5%)]\tLoss: 2.303281\n",
      "Step: 10176 Train Epoch: 43 [17920/60000 (29.8%)]\tLoss: 2.303643\n",
      "Step: 10186 Train Epoch: 43 [20480/60000 (34.0%)]\tLoss: 2.301376\n",
      "Step: 10196 Train Epoch: 43 [23040/60000 (38.3%)]\tLoss: 2.304263\n",
      "Step: 10206 Train Epoch: 43 [25600/60000 (42.6%)]\tLoss: 2.300472\n",
      "Step: 10216 Train Epoch: 43 [28160/60000 (46.8%)]\tLoss: 2.299856\n",
      "Step: 10226 Train Epoch: 43 [30720/60000 (51.1%)]\tLoss: 2.307542\n",
      "Step: 10236 Train Epoch: 43 [33280/60000 (55.3%)]\tLoss: 2.304162\n",
      "Step: 10246 Train Epoch: 43 [35840/60000 (59.6%)]\tLoss: 2.302402\n",
      "Step: 10256 Train Epoch: 43 [38400/60000 (63.8%)]\tLoss: 2.301626\n",
      "Step: 10266 Train Epoch: 43 [40960/60000 (68.1%)]\tLoss: 2.307183\n",
      "Step: 10276 Train Epoch: 43 [43520/60000 (72.3%)]\tLoss: 2.305140\n",
      "Step: 10286 Train Epoch: 43 [46080/60000 (76.6%)]\tLoss: 2.307564\n",
      "Step: 10296 Train Epoch: 43 [48640/60000 (80.9%)]\tLoss: 2.303912\n",
      "Step: 10306 Train Epoch: 43 [51200/60000 (85.1%)]\tLoss: 2.307460\n",
      "Step: 10316 Train Epoch: 43 [53760/60000 (89.4%)]\tLoss: 2.304746\n",
      "Step: 10326 Train Epoch: 43 [56320/60000 (93.6%)]\tLoss: 2.303016\n",
      "Step: 10336 Train Epoch: 43 [58880/60000 (97.9%)]\tLoss: 2.303239\n",
      "\n",
      "Test set: Average loss: 2.3028, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 44/50...\n",
      "Step: 10341 Train Epoch: 44 [0/60000 (0.0%)]\tLoss: 2.303033\n",
      "Step: 10351 Train Epoch: 44 [2560/60000 (4.3%)]\tLoss: 2.304354\n",
      "Step: 10361 Train Epoch: 44 [5120/60000 (8.5%)]\tLoss: 2.305685\n",
      "Step: 10371 Train Epoch: 44 [7680/60000 (12.8%)]\tLoss: 2.306375\n",
      "Step: 10381 Train Epoch: 44 [10240/60000 (17.0%)]\tLoss: 2.307779\n",
      "Step: 10391 Train Epoch: 44 [12800/60000 (21.3%)]\tLoss: 2.302727\n",
      "Step: 10401 Train Epoch: 44 [15360/60000 (25.5%)]\tLoss: 2.302214\n",
      "Step: 10411 Train Epoch: 44 [17920/60000 (29.8%)]\tLoss: 2.302204\n",
      "Step: 10421 Train Epoch: 44 [20480/60000 (34.0%)]\tLoss: 2.305979\n",
      "Step: 10431 Train Epoch: 44 [23040/60000 (38.3%)]\tLoss: 2.301298\n",
      "Step: 10441 Train Epoch: 44 [25600/60000 (42.6%)]\tLoss: 2.305095\n",
      "Step: 10451 Train Epoch: 44 [28160/60000 (46.8%)]\tLoss: 2.299837\n",
      "Step: 10461 Train Epoch: 44 [30720/60000 (51.1%)]\tLoss: 2.303155\n",
      "Step: 10471 Train Epoch: 44 [33280/60000 (55.3%)]\tLoss: 2.304422\n",
      "Step: 10481 Train Epoch: 44 [35840/60000 (59.6%)]\tLoss: 2.303744\n",
      "Step: 10491 Train Epoch: 44 [38400/60000 (63.8%)]\tLoss: 2.301598\n",
      "Step: 10501 Train Epoch: 44 [40960/60000 (68.1%)]\tLoss: 2.300414\n",
      "Step: 10511 Train Epoch: 44 [43520/60000 (72.3%)]\tLoss: 2.304826\n",
      "Step: 10521 Train Epoch: 44 [46080/60000 (76.6%)]\tLoss: 2.301666\n",
      "Step: 10531 Train Epoch: 44 [48640/60000 (80.9%)]\tLoss: 2.304424\n",
      "Step: 10541 Train Epoch: 44 [51200/60000 (85.1%)]\tLoss: 2.303469\n",
      "Step: 10551 Train Epoch: 44 [53760/60000 (89.4%)]\tLoss: 2.305306\n",
      "Step: 10561 Train Epoch: 44 [56320/60000 (93.6%)]\tLoss: 2.301347\n",
      "Step: 10571 Train Epoch: 44 [58880/60000 (97.9%)]\tLoss: 2.305825\n",
      "\n",
      "Test set: Average loss: 2.3040, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 45/50...\n",
      "Step: 10576 Train Epoch: 45 [0/60000 (0.0%)]\tLoss: 2.303453\n",
      "Step: 10586 Train Epoch: 45 [2560/60000 (4.3%)]\tLoss: 2.308719\n",
      "Step: 10596 Train Epoch: 45 [5120/60000 (8.5%)]\tLoss: 2.307702\n",
      "Step: 10606 Train Epoch: 45 [7680/60000 (12.8%)]\tLoss: 2.299824\n",
      "Step: 10616 Train Epoch: 45 [10240/60000 (17.0%)]\tLoss: 2.302208\n",
      "Step: 10626 Train Epoch: 45 [12800/60000 (21.3%)]\tLoss: 2.307119\n",
      "Step: 10636 Train Epoch: 45 [15360/60000 (25.5%)]\tLoss: 2.303902\n",
      "Step: 10646 Train Epoch: 45 [17920/60000 (29.8%)]\tLoss: 2.302696\n",
      "Step: 10656 Train Epoch: 45 [20480/60000 (34.0%)]\tLoss: 2.304705\n",
      "Step: 10666 Train Epoch: 45 [23040/60000 (38.3%)]\tLoss: 2.303232\n",
      "Step: 10676 Train Epoch: 45 [25600/60000 (42.6%)]\tLoss: 2.303749\n",
      "Step: 10686 Train Epoch: 45 [28160/60000 (46.8%)]\tLoss: 2.301695\n",
      "Step: 10696 Train Epoch: 45 [30720/60000 (51.1%)]\tLoss: 2.303184\n",
      "Step: 10706 Train Epoch: 45 [33280/60000 (55.3%)]\tLoss: 2.305046\n",
      "Step: 10716 Train Epoch: 45 [35840/60000 (59.6%)]\tLoss: 2.300498\n",
      "Step: 10726 Train Epoch: 45 [38400/60000 (63.8%)]\tLoss: 2.304231\n",
      "Step: 10736 Train Epoch: 45 [40960/60000 (68.1%)]\tLoss: 2.303339\n",
      "Step: 10746 Train Epoch: 45 [43520/60000 (72.3%)]\tLoss: 2.304344\n",
      "Step: 10756 Train Epoch: 45 [46080/60000 (76.6%)]\tLoss: 2.299510\n",
      "Step: 10766 Train Epoch: 45 [48640/60000 (80.9%)]\tLoss: 2.301215\n",
      "Step: 10776 Train Epoch: 45 [51200/60000 (85.1%)]\tLoss: 2.303705\n",
      "Step: 10786 Train Epoch: 45 [53760/60000 (89.4%)]\tLoss: 2.305764\n",
      "Step: 10796 Train Epoch: 45 [56320/60000 (93.6%)]\tLoss: 2.302823\n",
      "Step: 10806 Train Epoch: 45 [58880/60000 (97.9%)]\tLoss: 2.306183\n",
      "\n",
      "Test set: Average loss: 2.3031, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 46/50...\n",
      "Step: 10811 Train Epoch: 46 [0/60000 (0.0%)]\tLoss: 2.302661\n",
      "Step: 10821 Train Epoch: 46 [2560/60000 (4.3%)]\tLoss: 2.303064\n",
      "Step: 10831 Train Epoch: 46 [5120/60000 (8.5%)]\tLoss: 2.299767\n",
      "Step: 10841 Train Epoch: 46 [7680/60000 (12.8%)]\tLoss: 2.298543\n",
      "Step: 10851 Train Epoch: 46 [10240/60000 (17.0%)]\tLoss: 2.301764\n",
      "Step: 10861 Train Epoch: 46 [12800/60000 (21.3%)]\tLoss: 2.305039\n",
      "Step: 10871 Train Epoch: 46 [15360/60000 (25.5%)]\tLoss: 2.302782\n",
      "Step: 10881 Train Epoch: 46 [17920/60000 (29.8%)]\tLoss: 2.302919\n",
      "Step: 10891 Train Epoch: 46 [20480/60000 (34.0%)]\tLoss: 2.302929\n",
      "Step: 10901 Train Epoch: 46 [23040/60000 (38.3%)]\tLoss: 2.303665\n",
      "Step: 10911 Train Epoch: 46 [25600/60000 (42.6%)]\tLoss: 2.301513\n",
      "Step: 10921 Train Epoch: 46 [28160/60000 (46.8%)]\tLoss: 2.302530\n",
      "Step: 10931 Train Epoch: 46 [30720/60000 (51.1%)]\tLoss: 2.303906\n",
      "Step: 10941 Train Epoch: 46 [33280/60000 (55.3%)]\tLoss: 2.302970\n",
      "Step: 10951 Train Epoch: 46 [35840/60000 (59.6%)]\tLoss: 2.303617\n",
      "Step: 10961 Train Epoch: 46 [38400/60000 (63.8%)]\tLoss: 2.302678\n",
      "Step: 10971 Train Epoch: 46 [40960/60000 (68.1%)]\tLoss: 2.305834\n",
      "Step: 10981 Train Epoch: 46 [43520/60000 (72.3%)]\tLoss: 2.301814\n",
      "Step: 10991 Train Epoch: 46 [46080/60000 (76.6%)]\tLoss: 2.302189\n",
      "Step: 11001 Train Epoch: 46 [48640/60000 (80.9%)]\tLoss: 2.304355\n",
      "Step: 11011 Train Epoch: 46 [51200/60000 (85.1%)]\tLoss: 2.309630\n",
      "Step: 11021 Train Epoch: 46 [53760/60000 (89.4%)]\tLoss: 2.304602\n",
      "Step: 11031 Train Epoch: 46 [56320/60000 (93.6%)]\tLoss: 2.300648\n",
      "Step: 11041 Train Epoch: 46 [58880/60000 (97.9%)]\tLoss: 2.306791\n",
      "\n",
      "Test set: Average loss: 2.3042, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 47/50...\n",
      "Step: 11046 Train Epoch: 47 [0/60000 (0.0%)]\tLoss: 2.303186\n",
      "Step: 11056 Train Epoch: 47 [2560/60000 (4.3%)]\tLoss: 2.307267\n",
      "Step: 11066 Train Epoch: 47 [5120/60000 (8.5%)]\tLoss: 2.300343\n",
      "Step: 11076 Train Epoch: 47 [7680/60000 (12.8%)]\tLoss: 2.302734\n",
      "Step: 11086 Train Epoch: 47 [10240/60000 (17.0%)]\tLoss: 2.302221\n",
      "Step: 11096 Train Epoch: 47 [12800/60000 (21.3%)]\tLoss: 2.305162\n",
      "Step: 11106 Train Epoch: 47 [15360/60000 (25.5%)]\tLoss: 2.304541\n",
      "Step: 11116 Train Epoch: 47 [17920/60000 (29.8%)]\tLoss: 2.301457\n",
      "Step: 11126 Train Epoch: 47 [20480/60000 (34.0%)]\tLoss: 2.301549\n",
      "Step: 11136 Train Epoch: 47 [23040/60000 (38.3%)]\tLoss: 2.303743\n",
      "Step: 11146 Train Epoch: 47 [25600/60000 (42.6%)]\tLoss: 2.303822\n",
      "Step: 11156 Train Epoch: 47 [28160/60000 (46.8%)]\tLoss: 2.301992\n",
      "Step: 11166 Train Epoch: 47 [30720/60000 (51.1%)]\tLoss: 2.305702\n",
      "Step: 11176 Train Epoch: 47 [33280/60000 (55.3%)]\tLoss: 2.304637\n",
      "Step: 11186 Train Epoch: 47 [35840/60000 (59.6%)]\tLoss: 2.305052\n",
      "Step: 11196 Train Epoch: 47 [38400/60000 (63.8%)]\tLoss: 2.303294\n",
      "Step: 11206 Train Epoch: 47 [40960/60000 (68.1%)]\tLoss: 2.308388\n",
      "Step: 11216 Train Epoch: 47 [43520/60000 (72.3%)]\tLoss: 2.303967\n",
      "Step: 11226 Train Epoch: 47 [46080/60000 (76.6%)]\tLoss: 2.307561\n",
      "Step: 11236 Train Epoch: 47 [48640/60000 (80.9%)]\tLoss: 2.302787\n",
      "Step: 11246 Train Epoch: 47 [51200/60000 (85.1%)]\tLoss: 2.302407\n",
      "Step: 11256 Train Epoch: 47 [53760/60000 (89.4%)]\tLoss: 2.311598\n",
      "Step: 11266 Train Epoch: 47 [56320/60000 (93.6%)]\tLoss: 2.301574\n",
      "Step: 11276 Train Epoch: 47 [58880/60000 (97.9%)]\tLoss: 2.301774\n",
      "\n",
      "Test set: Average loss: 2.3028, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 48/50...\n",
      "Step: 11281 Train Epoch: 48 [0/60000 (0.0%)]\tLoss: 2.302789\n",
      "Step: 11291 Train Epoch: 48 [2560/60000 (4.3%)]\tLoss: 2.305021\n",
      "Step: 11301 Train Epoch: 48 [5120/60000 (8.5%)]\tLoss: 2.301901\n",
      "Step: 11311 Train Epoch: 48 [7680/60000 (12.8%)]\tLoss: 2.302426\n",
      "Step: 11321 Train Epoch: 48 [10240/60000 (17.0%)]\tLoss: 2.302967\n",
      "Step: 11331 Train Epoch: 48 [12800/60000 (21.3%)]\tLoss: 2.301824\n",
      "Step: 11341 Train Epoch: 48 [15360/60000 (25.5%)]\tLoss: 2.302391\n",
      "Step: 11351 Train Epoch: 48 [17920/60000 (29.8%)]\tLoss: 2.302465\n",
      "Step: 11361 Train Epoch: 48 [20480/60000 (34.0%)]\tLoss: 2.304227\n",
      "Step: 11371 Train Epoch: 48 [23040/60000 (38.3%)]\tLoss: 2.304414\n",
      "Step: 11381 Train Epoch: 48 [25600/60000 (42.6%)]\tLoss: 2.307014\n",
      "Step: 11391 Train Epoch: 48 [28160/60000 (46.8%)]\tLoss: 2.305146\n",
      "Step: 11401 Train Epoch: 48 [30720/60000 (51.1%)]\tLoss: 2.300621\n",
      "Step: 11411 Train Epoch: 48 [33280/60000 (55.3%)]\tLoss: 2.304130\n",
      "Step: 11421 Train Epoch: 48 [35840/60000 (59.6%)]\tLoss: 2.306629\n",
      "Step: 11431 Train Epoch: 48 [38400/60000 (63.8%)]\tLoss: 2.301944\n",
      "Step: 11441 Train Epoch: 48 [40960/60000 (68.1%)]\tLoss: 2.301373\n",
      "Step: 11451 Train Epoch: 48 [43520/60000 (72.3%)]\tLoss: 2.302891\n",
      "Step: 11461 Train Epoch: 48 [46080/60000 (76.6%)]\tLoss: 2.304680\n",
      "Step: 11471 Train Epoch: 48 [48640/60000 (80.9%)]\tLoss: 2.303908\n",
      "Step: 11481 Train Epoch: 48 [51200/60000 (85.1%)]\tLoss: 2.301028\n",
      "Step: 11491 Train Epoch: 48 [53760/60000 (89.4%)]\tLoss: 2.307234\n",
      "Step: 11501 Train Epoch: 48 [56320/60000 (93.6%)]\tLoss: 2.303164\n",
      "Step: 11511 Train Epoch: 48 [58880/60000 (97.9%)]\tLoss: 2.306373\n",
      "\n",
      "Test set: Average loss: 2.3029, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Start epoch 49/50...\n",
      "Step: 11516 Train Epoch: 49 [0/60000 (0.0%)]\tLoss: 2.302179\n",
      "Step: 11526 Train Epoch: 49 [2560/60000 (4.3%)]\tLoss: 2.300848\n",
      "Step: 11536 Train Epoch: 49 [5120/60000 (8.5%)]\tLoss: 2.301803\n",
      "Step: 11546 Train Epoch: 49 [7680/60000 (12.8%)]\tLoss: 2.301733\n",
      "Step: 11556 Train Epoch: 49 [10240/60000 (17.0%)]\tLoss: 2.304269\n",
      "Step: 11566 Train Epoch: 49 [12800/60000 (21.3%)]\tLoss: 2.302688\n",
      "Step: 11576 Train Epoch: 49 [15360/60000 (25.5%)]\tLoss: 2.303368\n",
      "Step: 11586 Train Epoch: 49 [17920/60000 (29.8%)]\tLoss: 2.303100\n",
      "Step: 11596 Train Epoch: 49 [20480/60000 (34.0%)]\tLoss: 2.305801\n",
      "Step: 11606 Train Epoch: 49 [23040/60000 (38.3%)]\tLoss: 2.307888\n",
      "Step: 11616 Train Epoch: 49 [25600/60000 (42.6%)]\tLoss: 2.302628\n",
      "Step: 11626 Train Epoch: 49 [28160/60000 (46.8%)]\tLoss: 2.306270\n",
      "Step: 11636 Train Epoch: 49 [30720/60000 (51.1%)]\tLoss: 2.299932\n",
      "Step: 11646 Train Epoch: 49 [33280/60000 (55.3%)]\tLoss: 2.303125\n",
      "Step: 11656 Train Epoch: 49 [35840/60000 (59.6%)]\tLoss: 2.300427\n",
      "Step: 11666 Train Epoch: 49 [38400/60000 (63.8%)]\tLoss: 2.302811\n",
      "Step: 11676 Train Epoch: 49 [40960/60000 (68.1%)]\tLoss: 2.303853\n",
      "Step: 11686 Train Epoch: 49 [43520/60000 (72.3%)]\tLoss: 2.302501\n",
      "Step: 11696 Train Epoch: 49 [46080/60000 (76.6%)]\tLoss: 2.302432\n",
      "Step: 11706 Train Epoch: 49 [48640/60000 (80.9%)]\tLoss: 2.301877\n",
      "Step: 11716 Train Epoch: 49 [51200/60000 (85.1%)]\tLoss: 2.301208\n",
      "Step: 11726 Train Epoch: 49 [53760/60000 (89.4%)]\tLoss: 2.303855\n",
      "Step: 11736 Train Epoch: 49 [56320/60000 (93.6%)]\tLoss: 2.303416\n",
      "Step: 11746 Train Epoch: 49 [58880/60000 (97.9%)]\tLoss: 2.301450\n",
      "\n",
      "Test set: Average loss: 2.3029, Accuracy: 1000/10000 (10.0000%)\n",
      "\n",
      "Best accuracy: 0.1\n",
      "Best accuracy: 0.1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Module.load_state_dict() missing 1 required positional argument: 'state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m path_refine \u001b[38;5;241m=\u001b[39m  path_sv \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpruned_grad.pth.tar\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m cfg_train \u001b[38;5;241m=\u001b[39m  load_config_train(lbd, path_sv, path_bckp, path_log)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mfit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m cfg_prune \u001b[38;5;241m=\u001b[39m load_config_prune(path_model, path_sv)\n\u001b[0;32m     11\u001b[0m main(cfg_prune)\n",
      "File \u001b[1;32mc:\\Users\\laeti\\SHK_NODE\\filter_sparsity\\polar_ns\\train.py:470\u001b[0m, in \u001b[0;36mfit_model\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    467\u001b[0m writer\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest accuracy: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(best_prec1))    \n\u001b[1;32m--> 470\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Module.load_state_dict() missing 1 required positional argument: 'state_dict'"
     ]
    }
   ],
   "source": [
    "for lbd in lambda_seq:\n",
    "    print(lbd)\n",
    "    path_sv, path_bckp, path_log = generate_paths(lbd)\n",
    "    path_model = path_sv + 'model_best.pth.tar',\n",
    "    path_refine =  path_sv + 'pruned_grad.pth.tar',\n",
    "    \n",
    "    cfg_train =  load_config_train(lbd, path_sv, path_bckp, path_log)\n",
    "    fit_model(cfg_train)\n",
    "    \n",
    "    cfg_prune = load_config_prune(path_model, path_sv)\n",
    "    main(cfg_prune)\n",
    "    cfg_finetune = load_config_finetune(path_sv, path_bckp, path_log, path_refine)\n",
    "    fine_tune_model(cfg_finetune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def repr_and_saves(seed, cuda = False, log = 'logs', save = 'results', backup_path = 'backups'):\n",
    "#     torch.manual_seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     if cuda:\n",
    "#         torch.cuda.manual_seed(seed)\n",
    "#         torch.backends.cudnn.deterministic = True\n",
    "#         torch.backends.cudnn.benchmark = False\n",
    "\n",
    "#     if not os.path.exists(save):\n",
    "#         os.makedirs(save)\n",
    "#     if backup_path is not None and not os.path.exists(backup_path):\n",
    "#         os.makedirs(backup_path)\n",
    "#     if not os.path.exists(log):\n",
    "#         os.makedirs(log)\n",
    "\n",
    "# def get_loaders(batch_size, test_batch_size): \n",
    "#     train_loader = torch.utils.data.DataLoader(\n",
    "#     datasets.FashionMNIST('./data.fashionMNIST', train=True, download=True,\n",
    "#                     transform=transforms.Compose([\n",
    "#                         transforms.Pad(2),\n",
    "#                         #transforms.RandomCrop(32),\n",
    "#                         #transforms.RandomHorizontalFlip(),\n",
    "#                         transforms.ToTensor(),\n",
    "#                         transforms.Normalize((0.5,), (0.5,))\n",
    "#                     ])),\n",
    "#     batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#     test_loader = torch.utils.data.DataLoader(\n",
    "#     datasets.FashionMNIST('./data.fashionMNIST', train=False, transform=transforms.Compose([\n",
    "#         transforms.Pad(2),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.5,), (0.5,))\n",
    "#     ])),\n",
    "#     batch_size=test_batch_size, shuffle=True)\n",
    "    \n",
    "#     return train_loader, test_loader\n",
    "\n",
    "\n",
    "# def freeze_sparse_gate(model: nn.Module):\n",
    "#     # do not update all SparseGate\n",
    "#     for sub_module in model.modules():\n",
    "#         if isinstance(sub_module, models.common.SparseGate):\n",
    "#             for p in sub_module.parameters():\n",
    "#                 # do not update SparseGate\n",
    "#                 p.requires_grad = False\n",
    "                \n",
    "# def define_optim(model, bn_wd, lr, momentum, weight_decay): \n",
    "#     if bn_wd:\n",
    "#         no_wd_type = [models.common.SparseGate]\n",
    "#     else:\n",
    "#         # do not apply weight decay on bn layers\n",
    "#         no_wd_type = [models.common.SparseGate, nn.BatchNorm2d, nn.BatchNorm1d]\n",
    "\n",
    "#     no_wd_params = []  # do not apply weight decay on these parameters\n",
    "#     for module_name, sub_module in model.named_modules():\n",
    "#         for t in no_wd_type:\n",
    "#             if isinstance(sub_module, t):\n",
    "#                 for param_name, param in sub_module.named_parameters():\n",
    "#                     no_wd_params.append(param)\n",
    "#                     print(f\"No weight decay param: module {module_name} param {param_name}\")\n",
    "\n",
    "#     no_wd_params_set = set(no_wd_params)  # apply weight decay on the rest of parameters\n",
    "#     wd_params = []\n",
    "#     for param_name, model_p in model.named_parameters():\n",
    "#         if model_p not in no_wd_params_set:\n",
    "#             wd_params.append(model_p)\n",
    "#             print(f\"Weight decay param: parameter name {param_name}\")\n",
    "\n",
    "#     optimizer = torch.optim.SGD([{'params': list(no_wd_params), 'weight_decay': 0.},\n",
    "#                                 {'params': list(wd_params), 'weight_decay': weight_decay}],\n",
    "#                                 lr,\n",
    "#                                 momentum=momentum)\n",
    "    \n",
    "#     return optimizer\n",
    "\n",
    "\n",
    "\n",
    "# def bn_weights(model):\n",
    "#     weights = []\n",
    "#     bias = []\n",
    "#     for name, m in model.named_modules():\n",
    "#         if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "#             weights.append((name, m.weight.data))\n",
    "#             bias.append((name, m.bias.data))\n",
    "\n",
    "#     return weights, bias\n",
    "#     pass\n",
    "\n",
    "\n",
    "# # def adjust_learning_rate(optimizer, epoch, gammas, schedule, config):\n",
    "# #     \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "# #     lr = config.get('lr')\n",
    "# #     assert len(gammas) == len(schedule), \"length of gammas and schedule should be equal\"\n",
    "# #     for (gamma, step) in zip(gammas, schedule):\n",
    "# #         if epoch >= step:\n",
    "# #             lr = lr * gamma\n",
    "# #         else:\n",
    "# #             break\n",
    "# #     for param_group in optimizer.param_groups:\n",
    "# #         param_group['lr'] = lr\n",
    "# #     return lr\n",
    "\n",
    "\n",
    "# # additional subgradient descent on the sparsity-induced penalty term\n",
    "# def updateBN(config, model):\n",
    "#     if config.get('loss') == LossType.L1_SPARSITY_REGULARIZATION:\n",
    "#         sparsity = config.get('lbd')\n",
    "#         bn_modules = list(filter(lambda m: (isinstance(m[1], nn.BatchNorm2d) or isinstance(m[1], nn.BatchNorm1d)),\n",
    "#                                  model.named_modules()))\n",
    "#         bn_modules = list(map(lambda m: m[1], bn_modules))  # remove module name\n",
    "#         for m in bn_modules:\n",
    "#             if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "#                 m.weight.grad.data.add_(sparsity * torch.sign(m.weight.data))\n",
    "#     else:\n",
    "#         raise NotImplementedError(f\"Do not support loss: {config.get('loss')}\")\n",
    "\n",
    "\n",
    "# def clamp_bn(model, lower_bound=0, upper_bound=1):\n",
    "#     if model.gate:\n",
    "#         sparse_modules = list(filter(lambda m: isinstance(m, SparseGate), model.modules()))\n",
    "#     else:\n",
    "#         sparse_modules = list(\n",
    "#             filter(lambda m: isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d), model.modules()))\n",
    "\n",
    "#     for m in sparse_modules:\n",
    "#         m.weight.data.clamp_(lower_bound, upper_bound)\n",
    "\n",
    "\n",
    "# def set_bn_zero(model: nn.Module, threshold=0.0):\n",
    "#     \"\"\"\n",
    "#     Set bn bias to zero\n",
    "#     Note: The operation is inplace. Parameters of the model will be changed!\n",
    "#     :param model: to set\n",
    "#     :param threshold: set bn bias to zero if corresponding lambda <= threshold\n",
    "#     :return modified model, the number of zero bn channels\n",
    "#     \"\"\"\n",
    "#     with torch.no_grad():\n",
    "#         mask_length = 0\n",
    "#         for name, sub_module in model.named_modules():\n",
    "#             # only process bn modules\n",
    "#             if not (isinstance(sub_module, nn.BatchNorm1d) or isinstance(sub_module, nn.BatchNorm2d)):\n",
    "#                 continue\n",
    "\n",
    "#             mask = sub_module.weight.detach() <= threshold\n",
    "#             sub_module.weight[mask] = 0.\n",
    "#             sub_module.bias[mask] = 0.\n",
    "\n",
    "#             mask_length += torch.sum(mask).item()\n",
    "\n",
    "#     return model, mask_length\n",
    "\n",
    "\n",
    "# def bn_sparsity(model, loss_type, sparsity, t, alpha,\n",
    "#                 flops_weighted: bool, weight_min=None, weight_max=None):\n",
    "#     \"\"\"\n",
    "\n",
    "#     :type model: torch.nn.Module\n",
    "#     :type alpha: float\n",
    "#     :type t: float\n",
    "#     :type sparsity: float\n",
    "#     :type loss_type: LossType\n",
    "#     \"\"\"\n",
    "#     bn_modules = model.get_sparse_layers()\n",
    "\n",
    "#     if loss_type == LossType.POLARIZATION or loss_type == LossType.L2_POLARIZATION:\n",
    "#         # compute global mean of all sparse vectors\n",
    "#         n_ = sum(map(lambda m: m.weight.data.shape[0], bn_modules))\n",
    "#         sparse_weights_mean = torch.sum(torch.stack(list(map(lambda m: torch.sum(m.weight), bn_modules)))) / n_\n",
    "\n",
    "#         sparsity_loss = 0.\n",
    "#         if flops_weighted:\n",
    "#             for sub_module in model.modules():\n",
    "#                 if isinstance(sub_module, model.building_block):\n",
    "#                     flops_weight = sub_module.get_conv_flops_weight(update=True, scaling=True)\n",
    "#                     sub_module_sparse_layers = sub_module.get_sparse_modules()\n",
    "\n",
    "#                     for sparse_m, flops_w in zip(sub_module_sparse_layers, flops_weight):\n",
    "#                         # linear rescale the weight from [0, 1] to [lambda_min, lambda_max]\n",
    "#                         flops_w = weight_min + (weight_max - weight_min) * flops_w\n",
    "\n",
    "#                         sparsity_term = t * torch.sum(torch.abs(sparse_m.weight.view(-1))) - torch.sum(\n",
    "#                             torch.abs(sparse_m.weight.view(-1) - alpha * sparse_weights_mean))\n",
    "#                         sparsity_loss += flops_w * sparsity * sparsity_term\n",
    "#             return sparsity_loss\n",
    "#         else:\n",
    "#             for m in bn_modules:\n",
    "#                 if loss_type == LossType.POLARIZATION:\n",
    "#                     sparsity_term = t * torch.sum(torch.abs(m.weight)) - torch.sum(\n",
    "#                         torch.abs(m.weight - alpha * sparse_weights_mean))\n",
    "#                 elif loss_type == LossType.L2_POLARIZATION:\n",
    "#                     sparsity_term = t * torch.sum(torch.abs(m.weight)) - torch.sum(\n",
    "#                         (m.weight - alpha * sparse_weights_mean) ** 2)\n",
    "#                 else:\n",
    "#                     raise ValueError(f\"Unexpected loss type: {loss_type}\")\n",
    "#                 sparsity_loss += sparsity * sparsity_term\n",
    "\n",
    "#             return sparsity_loss\n",
    "#     else:\n",
    "#         raise ValueError()\n",
    "\n",
    "\n",
    "# def train(model, epoch, train_loader, optimizer, lr_scheduler, config, history_score, global_step):\n",
    "#     model.train()\n",
    "#     #global history_score, global_step \n",
    "#     avg_loss = 0.\n",
    "#     avg_sparsity_loss = 0.\n",
    "#     train_acc = 0.\n",
    "#     total_data = 0\n",
    "#     for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#         if config.get('cuda'):\n",
    "#             data, target = data.cuda(), target.cuda()\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(data)\n",
    "#         if isinstance(output, tuple):\n",
    "#             output, output_aux = output\n",
    "#         loss = F.cross_entropy(output, target)\n",
    "\n",
    "#         # logging\n",
    "#         avg_loss += loss.data.item()\n",
    "#         pred = output.data.max(1, keepdim=True)[1]\n",
    "#         train_acc += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "#         total_data += target.data.shape[0]\n",
    "\n",
    "#         if config.get('loss') in {LossType.POLARIZATION,\n",
    "#                          LossType.L2_POLARIZATION}:\n",
    "#             sparsity_loss = bn_sparsity(model, config.get('loss'), config.get('lbd'),\n",
    "#                                         t=config.get('t'), alpha=config.get('alpha'),\n",
    "#                                         flops_weighted=config.get('flops_weighted'),\n",
    "#                                         weight_max=config.get('weight_max'), weight_min=config.get('weight_min'))\n",
    "#             loss += sparsity_loss\n",
    "#             avg_sparsity_loss += sparsity_loss.data.item()\n",
    "#         loss.backward()\n",
    "        \n",
    "#         if config.get('loss') in {LossType.L1_SPARSITY_REGULARIZATION}:\n",
    "#             updateBN(config, model)\n",
    "            \n",
    "#         optimizer.step()\n",
    "#         if config.get('loss') in {LossType.POLARIZATION,\n",
    "#                          LossType.L2_POLARIZATION}:\n",
    "#             clamp_bn(model, upper_bound=config.get('clamp'))\n",
    "#         global_step += 1\n",
    "        \n",
    "#         lr_scheduler.step()\n",
    "        \n",
    "#         if batch_idx % config.get('log_interval') == 0:\n",
    "#             print('Step: {} Train Epoch: {} [{}/{} ({:.1f}%)]\\tLoss: {:.6f}'.format(\n",
    "#                 global_step, epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "#                                     100. * batch_idx / len(train_loader), loss.data.item()))\n",
    "\n",
    "#     history_score[epoch][0] = avg_loss / len(train_loader)\n",
    "#     history_score[epoch][1] = float(train_acc) / float(total_data)\n",
    "#     history_score[epoch][3] = avg_sparsity_loss / len(train_loader)\n",
    "#     return history_score, global_step\n",
    "\n",
    "\n",
    "# def test(model, test_loader, config):\n",
    "#     model.eval()\n",
    "#     test_loss = 0\n",
    "#     correct = 0\n",
    "#     with torch.no_grad():\n",
    "#         for data, target in test_loader:\n",
    "#             if config.get('cuda'):\n",
    "#                 data, target = data.cuda(), target.cuda()\n",
    "#             output = model(data)\n",
    "#             if isinstance(output, tuple):\n",
    "#                 output, output_aux = output\n",
    "#             test_loss += F.cross_entropy(output, target, size_average=False).data.item()  # sum up batch loss\n",
    "#             pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "#             correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "#     test_loss /= len(test_loader.dataset)\n",
    "#     print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.4f}%)\\n'.format(\n",
    "#         test_loss, correct, len(test_loader.dataset),\n",
    "#         100. * float(correct) / len(test_loader.dataset)))\n",
    "#     return float(correct) / float(len(test_loader.dataset))\n",
    "\n",
    "\n",
    "# def save_checkpoint(state, is_best, filepath, backup: bool, backup_path: str, epoch: int, max_backup: int, config):\n",
    "#     state['config'] = config\n",
    "\n",
    "#     torch.save(state, os.path.join(filepath, 'checkpoint.pth.tar'))\n",
    "#     if is_best:\n",
    "#         shutil.copyfile(os.path.join(filepath, 'checkpoint.pth.tar'), os.path.join(filepath, 'model_best.pth.tar'))\n",
    "#     if backup and backup_path is not None:\n",
    "#         shutil.copyfile(os.path.join(filepath, 'checkpoint.pth.tar'),\n",
    "#                         os.path.join(backup_path, 'checkpoint_{}.pth.tar'.format(epoch)))\n",
    "\n",
    "#         if max_backup is not None:\n",
    "#             while True:\n",
    "#                 # remove redundant backup checkpoints to save space\n",
    "#                 checkpoint_match = map(lambda f_name: re.fullmatch(\"checkpoint_([0-9]+).pth.tar\", f_name),\n",
    "#                                        os.listdir(backup_path))\n",
    "#                 checkpoint_match = filter(lambda m: m is not None, checkpoint_match)\n",
    "#                 checkpoint_id: typing.List[int] = list(map(lambda m: int(m.group(1)), checkpoint_match))\n",
    "#                 checkpoint_count = len(checkpoint_id)\n",
    "#                 if checkpoint_count > max_backup:\n",
    "#                     min_checkpoint_epoch = min(checkpoint_id)\n",
    "#                     min_checkpoint_path = os.path.join(backup_path,\n",
    "#                                                        'checkpoint_{}.pth.tar'.format(min_checkpoint_epoch))\n",
    "#                     print(f\"Too much checkpoints (Max {max_backup}, got {checkpoint_count}).\")\n",
    "#                     print(f\"Remove file: {min_checkpoint_path}\")\n",
    "#                     os.remove(min_checkpoint_path)\n",
    "#                 else:\n",
    "#                     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fit_model(config): \n",
    "#     config['cuda'] = not config.get('no_cuda') and torch.cuda.is_available()\n",
    "#     config['loss'] = LossType.from_string(config.get('loss'))\n",
    "#     #config.get('decay_epoch') = sorted([int(config.get('epochs') * i if i < 1 else i) for i in config.get('decay_epoch')])\n",
    "#     if not config.get('seed'):\n",
    "#         config['seed'] = random.randint(500, 1000)\n",
    "    \n",
    "#     repr_and_saves(config.get('seed'))\n",
    "\n",
    "#     train_loader, test_loader = get_loaders(config.get('batch_size'), config.get('test_batch_size'))\n",
    "#     num_classes = 10 \n",
    "    \n",
    "    \n",
    "#     if not config.get('retrain'):\n",
    "#         model = LeNet5(gate=config.get('gate'), bn_init_value=config.get('bn_init_value'))\n",
    "#     else:  # initialize model for retraining with configs\n",
    "#         checkpoint = torch.load(config.get('retrain'))\n",
    "#         if config.get('arch') == \"leNet\":\n",
    "#             model = models.__dict__[config.get('arch')](num_classes=num_classes, cfg=checkpoint['cfg'])\n",
    "#         else:\n",
    "#             raise NotImplementedError(f\"Do not support {config.get('arch')} for retrain.\")\n",
    "        \n",
    "#     if config.get('fix_gate'):\n",
    "#         if config.get('lbd') != 0:\n",
    "#             raise ValueError(\"The lambda must be 0 in fix-gate mode.\")\n",
    "#         # do not update all SparseGate\n",
    "#         freeze_sparse_gate(model)\n",
    "        \n",
    "#     optimizer = define_optim(model, config.get('bn_wd'), config.get('lr'), config.get('momentum'), config.get('weight_decay'))\n",
    "#     lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 100, eta_min=0.0, last_epoch=-1, verbose='deprecated')\n",
    "    \n",
    "#     best_prec1 = 0.0\n",
    "#     global_step = 0\n",
    "#     writer = SummaryWriter(logdir=config.get('log', 'logs'))\n",
    "#     history_score = np.zeros((config.get('epochs'), 6))\n",
    "\n",
    "#     for epoch in range(config.get('start_epoch', 0), config.get('epochs')):\n",
    "#         if config.get('max_epoch') is not None and epoch >= config.get('max_epoch'):\n",
    "#             break\n",
    "\n",
    "#         #current_learning_rate = adjust_learning_rate(optimizer, epoch, config.get('gammas'), config.get('decay_epoch'), config)\n",
    "#         print(\"Start epoch {}/{}...\".format(epoch, config.get('epochs')))\n",
    "\n",
    "#         #weights, bias = bn_weights(model)\n",
    "        \n",
    "#         weights, bias = bn_weights(model)\n",
    "#         for bn_name, bn_weight in weights:\n",
    "#             writer.add_histogram(\"bn/\" + bn_name, bn_weight, global_step=epoch)\n",
    "#         for bn_name, bn_bias in bias:\n",
    "#             writer.add_histogram(\"bn_bias/\" + bn_name, bn_bias, global_step=epoch)\n",
    "#         # visualize conv kernels\n",
    "#         for name, sub_modules in model.named_modules():\n",
    "#             if isinstance(sub_modules, nn.Conv2d):\n",
    "#                 writer.add_histogram(\"conv_kernels/\" + name, sub_modules.weight, global_step=epoch)\n",
    "#         if config['gate']:\n",
    "#             for gate_name, m in model.named_modules():\n",
    "#                 if isinstance(m, SparseGate):\n",
    "#                     writer.add_histogram(\"gate/\" + gate_name, m.weight, global_step=epoch)\n",
    "\n",
    "        \n",
    "#         history_score, global_step = train(model, epoch, train_loader, optimizer, lr_scheduler, config, history_score, global_step)\n",
    "\n",
    "#         prec1 = test(model, test_loader, config)\n",
    "#         history_score[epoch][2] = prec1\n",
    "#         np.savetxt(os.path.join(config.get('save'), 'record.txt'), history_score, fmt='%10.5f', delimiter=',')\n",
    "#         is_best = prec1 > best_prec1\n",
    "#         best_prec1 = max(prec1, best_prec1)\n",
    "#         save_checkpoint({\n",
    "#             'epoch': epoch + 1,\n",
    "#             'state_dict': model.state_dict(),\n",
    "#             'best_prec1': best_prec1,\n",
    "#             'optimizer': optimizer.state_dict(),\n",
    "#         }, is_best, filepath=config.get('save', 'results'),\n",
    "#             backup_path=config.get('backup_path', 'backups'),\n",
    "#             backup=epoch % config.get('backup_freq', 10) == 0,\n",
    "#             epoch=epoch,\n",
    "#             max_backup=config.get('max_backup', 25), \n",
    "#             config = config\n",
    "#         )\n",
    "        \n",
    "        \n",
    "#         # write the tensorboard\n",
    "#         writer.add_scalar(\"train/average_loss\", history_score[epoch][0], epoch)\n",
    "#         writer.add_scalar(\"train/sparsity_loss\", history_score[epoch][3], epoch)\n",
    "#         writer.add_scalar(\"train/train_acc\", history_score[epoch][1], epoch)\n",
    "#         writer.add_scalar(\"train/lr\", optimizer.param_groups[0]['lr'], epoch)\n",
    "#         writer.add_scalar(\"val/acc\", prec1, epoch)\n",
    "#         writer.add_scalar(\"val/best_acc\", best_prec1, epoch)\n",
    "\n",
    "\n",
    "#         # flops\n",
    "#         # if config.get('loss') in {LossType.POLARIZATION, LossType.L2_POLARIZATION}:\n",
    "#         #     flops_grad, flops_fixed, baseline_flops = prune_while_training(model, arch=config.get('arch'),\n",
    "#         #                                                                 prune_mode=\"default\",\n",
    "#         #                                                                 num_classes=num_classes)\n",
    "#         #     print(f\" --> FLOPs in epoch (grad) {epoch}: {flops_grad:,}, ratio: {flops_grad / baseline_flops}\")\n",
    "#         #     print(f\" --> FLOPs in epoch (fixed) {epoch}: {flops_fixed:,}, ratio: {flops_fixed / baseline_flops}\")\n",
    "#         #     if config.get('loss') == LossType.POLARIZATION and config.get('target_flops') and (\n",
    "#         #             flops_grad / baseline_flops) <= config.get('target_flops') and config.get('gate'):\n",
    "#         #         print(\"The grad pruning FLOPs archieve the target FLOPs.\")\n",
    "#         #         print(f\"Current pruning ratio: {flops_grad / baseline_flops}\")\n",
    "#         #         print(\"Stop polarization from current epoch and continue training.\")\n",
    "\n",
    "#         #         # do not apply polarization loss\n",
    "#         #         config['lbd'] = 0\n",
    "#         #         freeze_sparse_gate(model)\n",
    "#         #         if config.get('backup_freq') > 20:\n",
    "#         #             config['backup_freq'] = 20\n",
    "\n",
    "#     # if config.get('loss') == LossType.POLARIZATION and config.get('target_flops') and (\n",
    "#     #         flops_grad / baseline_flops) > config.get('target_flops') and config.get('gate'):\n",
    "#     #     print(\"WARNING: the FLOPs does not achieve the target FLOPs at the end of training.\")\n",
    "#     print(\"Best accuracy: \" + str(best_prec1))\n",
    "#     history_score[-1][0] = best_prec1\n",
    "#     np.savetxt(os.path.join(config.get('save'), 'record.txt'), history_score, fmt='%10.5f', delimiter=',')\n",
    "\n",
    "#     writer.close()\n",
    "\n",
    "#     print(\"Best accuracy: \" + str(best_prec1))    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
